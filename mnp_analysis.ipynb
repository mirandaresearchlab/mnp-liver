{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import re\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = \"/home/jen-hungwang/Documents/mnp-liver/results\"\n",
    "save_dir = \"/home/jovyan/work/outputs/\" # Adjusted for Docker environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# csv_dir = Path(\"/home/jen-hungwang/Documents/MNP\")\n",
    "csv_dir = Path(\"/home/jovyan/work/data\")  # Adjusted for Docker environment\n",
    "hep_path = csv_dir / \"hep\" # hep or huh\n",
    "csv_data = \"df_SingleCell_AO_HEPG2_102912.csv\" # \"df_SingleCell_AO_HEPG2_102912.csv\", \"df_SingleCell_AO_HEPG2_110341.csv\", \"df_SingleCell_AO_HEPG2_231222.csv\"\n",
    "f = hep_path / csv_data\n",
    "\n",
    "# f1 = hep_path / \"df_SingleCell_AO_HEPG2_102912.csv\"\n",
    "# f2 = hep_path / \"df_SingleCell_AO_HEPG2_110341.csv\"\n",
    "# f3 = hep_path / \"df_SingleCell_AO_HEPG2_231222.csv\"\n",
    "\n",
    "# df1 = pd.read_csv(f1, sep=\",\", header=0)\n",
    "df = pd.read_csv(f, sep=\",\", header=0)\n",
    "# df3 = pd.read_csv(f3, sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Print first values of selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns of interest\n",
    "columns_of_interest = [\n",
    "    'Cells_AreaShape_NormalizedMoment_0_0',\n",
    "    'Cells_AreaShape_NormalizedMoment_0_1',\n",
    "    'Cells_AreaShape_NormalizedMoment_1_0',\n",
    "    'Nuclei_AreaShape_NormalizedMoment_0_0',\n",
    "    'Nuclei_AreaShape_NormalizedMoment_0_1',\n",
    "    'Nuclei_AreaShape_NormalizedMoment_1_0'\n",
    "]\n",
    "\n",
    "# Print first 10 values for specified columns\n",
    "print(\"\\nFirst 10 values for specified columns:\")\n",
    "for col in columns_of_interest:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df[col].head(10).to_list())\n",
    "    else:\n",
    "        print(f\"\\n{col}: Not found in DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectnumber_columns = [col for col in df.columns if '_ObjectNumber' in col]\n",
    "print(objectnumber_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly drop NC for data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'Metadata_concentration_perliter' \n",
    "# column_name = 'Metadata_compound' \n",
    "# column_name = 'Metadata_control_type'\n",
    "\n",
    "# Standardize '0' values (convert numeric 0 or similar to string '0')\n",
    "# df[column_name] = df[column_name].apply(lambda x: '0' if pd.to_numeric(x, errors='coerce') == 0 else x)\n",
    "\n",
    "value_counts = df[column_name].value_counts()\n",
    "print(f\"Unique values and their counts in column '{column_name}':\")\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame and column_name is defined\n",
    "column_name = 'Metadata_concentration_perliter'\n",
    "\n",
    "# Define the fraction to keep (%)\n",
    "percentage_to_keep = 38\n",
    "\n",
    "# Standardize '0' values (convert numeric 0 or similar to string '0')\n",
    "df[column_name] = df[column_name].apply(lambda x: '0' if pd.to_numeric(x, errors='coerce') == 0 else x)\n",
    "\n",
    "# Identify rows where Metadata_concentration_perliter == '0'\n",
    "zero_rows = df[df[column_name] == '0']\n",
    "\n",
    "# Calculate number of rows to keep\n",
    "rows_to_keep = int(len(zero_rows) * percentage_to_keep / 100)\n",
    "print(f\"Number of rows with {column_name} == '0': {len(zero_rows)}\")\n",
    "print(f\"Number of rows to keep ({percentage_to_keep:.2f}% of zeros): {rows_to_keep}\")\n",
    "\n",
    "# Randomly sample the zero rows to keep\n",
    "zero_rows_to_keep = zero_rows.sample(n=rows_to_keep, random_state=42)\n",
    "\n",
    "# Get rows where Metadata_concentration_perliter != '0'\n",
    "non_zero_rows = df[df[column_name] != '0']\n",
    "\n",
    "# Combine the kept zero rows with non-zero rows\n",
    "df_filtered = pd.concat([zero_rows_to_keep, non_zero_rows])\n",
    "\n",
    "# Reset index if needed\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "# Verify the new counts\n",
    "value_counts = df_filtered[column_name].value_counts()\n",
    "print(f\"Unique values and their counts in column '{column_name}' after dropping {100 - percentage_to_keep:.2f}% of zeros:\")\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, nan_threshold=0.05):\n",
    "    # Select feature columns\n",
    "    feature_columns = [col for col in df.columns if not col.startswith(('Metadata_', 'Image_')) and not col.endswith('_ObjectNumber')]\n",
    "    # print(\"Selected feature columns:\", feature_columns)\n",
    "    print(\"Number of feature columns:\", len(feature_columns))\n",
    "    \n",
    "    # Extract features\n",
    "    X = df[feature_columns]\n",
    "    \n",
    "    # Calculate NaN and Inf counts\n",
    "    nan_counts = X.isna().sum()\n",
    "    inf_counts = np.isinf(X).sum()\n",
    "    \n",
    "    # Identify columns with at least one NaN or Inf\n",
    "    columns_with_nan_or_inf = nan_counts[nan_counts > 0].index.union(inf_counts[inf_counts > 0].index)\n",
    "    \n",
    "    if len(columns_with_nan_or_inf) > 0:\n",
    "        print(\"\\nColumns with at least one NaN or Inf value:\")\n",
    "        print(\"\\n{:<60} {:>10} {:>10}\".format(\"Column\", \"NaN Count\", \"Inf Count\"))\n",
    "        print(\"-\" * 80)\n",
    "        for col in columns_with_nan_or_inf:\n",
    "            print(\"{:<60} {:>10} {:>10}\".format(col, nan_counts[col], inf_counts[col]))\n",
    "        print(f\"Total columns with NaN or Inf: {len(columns_with_nan_or_inf)}\")\n",
    "    \n",
    "    # Filter columns based on NaN and Inf threshold\n",
    "    threshold = X.shape[0] * nan_threshold\n",
    "    valid_columns = [col for col in X.columns if X[col].isna().sum() < threshold and np.isinf(X[col]).sum() < threshold]\n",
    "    # print(\"\\nValid columns after filtering (>50% valid data):\", valid_columns)\n",
    "    print(\"Number of valid columns:\", len(valid_columns))\n",
    "    \n",
    "    if not valid_columns:\n",
    "        raise ValueError(\"No valid columns remain after filtering.\")\n",
    "    \n",
    "    # Select valid columns\n",
    "    X = X[valid_columns]\n",
    "    \n",
    "    # Replace inf with NaN\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Fill NaN with median\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    # Check for remaining NaN values\n",
    "    nan_count_after_fill = X.isna().sum().sum()\n",
    "    print(\"\\nNaN count after filling with median:\", nan_count_after_fill)\n",
    "    if nan_count_after_fill > 0:\n",
    "        print(\"Warning: Some NaN values remain. Filling with zero.\")\n",
    "        X = X.fillna(0)\n",
    "    \n",
    "    # Check if data is valid\n",
    "    if X.shape[0] == 0 or X.shape[1] == 0:\n",
    "        raise ValueError(\"No rows/columns remain after preprocessing.\")\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, valid_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess each DataFrame\n",
    "# dataframes = {'df1': df1, 'df2': df2, 'df3': df3}\n",
    "# dataframes = {'df': df}\n",
    "dataframes = {'df': df_filtered}\n",
    "preprocessed_data = {}\n",
    "\n",
    "# print(f\"Original df1 shape: {df1.shape}\")\n",
    "# print(f\"Original df2 shape: {df2.shape}\")\n",
    "# print(f\"Original df3 shape: {df3.shape}\")\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"Original {name} shape: {df.shape}\")\n",
    "    print(f\"Preprocessing {name}...\")\n",
    "    X_scaled, valid_columns = preprocess_dataframe(df)\n",
    "    preprocessed_data[name] = {'X_scaled': X_scaled, 'valid_columns': valid_columns, 'df': df}\n",
    "    print(f\"Preprocessed {name} with {len(valid_columns)} valid columns.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D & 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dimensionality_reduction(X_scaled, df, valid_columns, metadata_column, method_name, title, tsne_perplexity=30, n_neighbors=15, min_dist=0.1, continuous=True, n_components=2, save_path=None):\n",
    "    if method_name == 'PCA':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        x_label = 'PC1'\n",
    "        y_label = 'PC2'\n",
    "        z_label = 'PC3' if n_components == 3 else None\n",
    "    elif method_name == 't-SNE':\n",
    "        reducer = TSNE(n_components=n_components, perplexity=tsne_perplexity, learning_rate='auto', random_state=42)\n",
    "        x_label = 't-SNE 1'\n",
    "        y_label = 't-SNE 2'\n",
    "        z_label = 't-SNE 3' if n_components == 3 else None\n",
    "    elif method_name == 'UMAP':\n",
    "        reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=42)\n",
    "        x_label = 'UMAP 1'\n",
    "        y_label = 'UMAP 2'\n",
    "        z_label = 'UMAP 3' if n_components == 3 else None\n",
    "    elif method_name == 'LDA':\n",
    "        labels = df[metadata_column].astype(str)\n",
    "        reducer = LDA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "        x_label = 'LD1'\n",
    "        y_label = 'LD2'\n",
    "        z_label = 'LD3' if n_components == 3 else None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method\")\n",
    "    \n",
    "    if method_name != 'LDA':\n",
    "        X_reduced = reducer.fit_transform(X_scaled)\n",
    "    \n",
    "    # 2D Plot\n",
    "    if n_components == 2:\n",
    "        fig_2d = go.Figure()\n",
    "        if continuous:\n",
    "            concentrations = df[metadata_column].apply(convert_concentration)\n",
    "            fig_2d.add_trace(go.Scatter(x=X_reduced[:, 0], y=X_reduced[:, 1], mode='markers',\n",
    "                                        marker=dict(color=concentrations, colorscale='Viridis', showscale=True, colorbar=dict(title=f'{metadata_column} (g)'))))\n",
    "            title += ' (Continuous)'\n",
    "        else:\n",
    "            labels = df[metadata_column].astype(str)\n",
    "            unique_labels = sorted(labels.unique(), key=convert_concentration, reverse=True)\n",
    "            color_map = {label: f'rgb({r}, {g}, {b})' for label, (r, g, b) in zip(unique_labels, sns.color_palette('tab10', len(unique_labels)))}\n",
    "            for label in unique_labels:\n",
    "                mask = labels == label\n",
    "                fig_2d.add_trace(go.Scatter(\n",
    "                    x=X_reduced[mask, 0], \n",
    "                    y=X_reduced[mask, 1], \n",
    "                    mode='markers',\n",
    "                    marker=dict(color=color_map[label], size=3),\n",
    "                    name=str(label),\n",
    "                    showlegend=True\n",
    "                ))\n",
    "            fig_2d.update_layout(showlegend=True, legend=dict(itemsizing='constant'))\n",
    "            title += ' (Categorical)'\n",
    "        fig_2d.update_layout(title=title + f' (2D, {len(valid_columns)} features)', xaxis_title=x_label, yaxis_title=y_label)\n",
    "        fig_2d.show()\n",
    "        # Save 2D plot as PNG if save_path is provided\n",
    "        if save_path:\n",
    "            fig_2d.write_image(f\"{save_path}_2D.png\", width=1200, height=800)\n",
    "    \n",
    "    # 3D Plot (if n_components = 3)\n",
    "    if n_components == 3:\n",
    "        fig_3d = go.Figure()\n",
    "        if continuous:\n",
    "            concentrations = df[metadata_column].apply(convert_concentration)\n",
    "            fig_3d.add_trace(go.Scatter3d(x=X_reduced[:, 0], y=X_reduced[:, 1], z=X_reduced[:, 2], mode='markers',\n",
    "                                          marker=dict(color=concentrations, colorscale='Viridis', showscale=True, colorbar=dict(title=f'{metadata_column} (g)'))))\n",
    "            title += ' (Continuous)'\n",
    "        else:\n",
    "            labels = df[metadata_column].astype(str)\n",
    "            unique_labels = sorted(labels.unique(), key=convert_concentration, reverse=True)\n",
    "            color_map = {label: f'rgb({r}, {g}, {b})' for label, (r, g, b) in zip(unique_labels, sns.color_palette('tab10', len(unique_labels)))}\n",
    "            for label in unique_labels:\n",
    "                mask = labels == label\n",
    "                fig_3d.add_trace(go.Scatter3d(\n",
    "                    x=X_reduced[mask, 0], \n",
    "                    y=X_reduced[mask, 1], \n",
    "                    z=X_reduced[mask, 2], \n",
    "                    mode='markers',\n",
    "                    marker=dict(color=color_map[label], size=3),\n",
    "                    name=str(label),\n",
    "                    showlegend=True\n",
    "                ))\n",
    "            fig_3d.update_layout(showlegend=True, legend=dict(itemsizing='constant'))\n",
    "            title += ' (Categorical)'\n",
    "        fig_3d.update_layout(title=title + f' (3D, {len(valid_columns)} features)', \n",
    "                             scene=dict(xaxis_title=x_label, yaxis_title=y_label, zaxis_title=z_label))\n",
    "        fig_3d.show()\n",
    "        # Save 3D plot as PNG if save_path is provided\n",
    "        if save_path:\n",
    "            fig_3d.write_image(f\"{save_path}_3D.png\", width=1200, height=800)\n",
    "            \n",
    "    return fig_2d if n_components == 2 else fig_3d\n",
    "    \n",
    "    # Function to convert concentration strings to numerical values (grams)\n",
    "def convert_concentration(value):\n",
    "    if pd.isna(value) or value == \"\":\n",
    "        return 0.0\n",
    "    try:\n",
    "        value = str(value).lower().replace(\" \", \"\")  # Normalize input\n",
    "        # Use regex to extract number and unit\n",
    "        match = re.match(r'(\\d*\\.?\\d+)([mnu]?g)?', value)\n",
    "        if match:\n",
    "            num = float(match.group(1))\n",
    "            unit = match.group(2) or ''\n",
    "            if unit == 'mg':\n",
    "                return num * 1e-3  # Convert mg to g\n",
    "            elif unit == 'ug':\n",
    "                return num * 1e-6  # Convert ug to g\n",
    "            elif unit == 'ng':\n",
    "                return num * 1e-9  # Convert ng to g\n",
    "            elif unit == 'g' or not unit:  # Includes pure numbers or 'g'\n",
    "                return num\n",
    "        return float(value)  # Fallback for pure numbers\n",
    "    except ValueError:\n",
    "        return 0.0  # Default to 0 for invalid entries\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction methods to each DataFrame\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "method = 'PCA'\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        # 2D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=2)\n",
    "        plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=2)\n",
    "        \n",
    "        # 3D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=3)\n",
    "        plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=3)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction methods to each DataFrame\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "method = 't-SNE'\n",
    "# save_dir = '/Users/jen-hung/Desktop/KTH/mnp-liver/results/'\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        # 2D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=2)\n",
    "        plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=2, \n",
    "                                      save_path=save_dir + f\"{csv_data}_{method}\")\n",
    "        \n",
    "        # 3D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=3)\n",
    "        plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=3, \n",
    "                                      save_path=save_dir + f\"{csv_data}_{method}\")\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction methods to each DataFrame\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "method = 'UMAP'\n",
    "# save_dir = '/Users/jen-hung/Desktop/KTH/mnp-liver/results/'\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        # 2D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=2)\n",
    "        plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=2, \n",
    "                                      save_path=save_dir + f\"{csv_data}_{method}\")\n",
    "        \n",
    "        # 3D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=3)\n",
    "        plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=3, \n",
    "                                      save_path=save_dir + f\"{csv_data}_{method}\")\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction methods to each DataFrame\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "method = 'LDA'\n",
    "# save_dir = \"/home/jen-hungwang/Desktop/\"\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        # 2D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=2)\n",
    "        plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=2, \n",
    "                                      save_path=save_dir + f\"{csv_data}_{method}\")\n",
    "        \n",
    "        # 3D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=3)\n",
    "        fig_original = plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=3,\n",
    "                                      save_path=save_dir + f\"{csv_data}_{method}\")\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-tune dimensionality reduction methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction methods to each DataFrame\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "method = 't-SNE'\n",
    "perplexity = [2, 5, 10, 20, 30, 50, 100]\n",
    "save_dir = \"/home/jen-hungwang/Desktop/t-sne/\"\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    for p in perplexity:\n",
    "        if metadata_column in data['df'].columns:\n",
    "            print(f\"Hyperparameters: perplexity={p}\")\n",
    "            # 2D plots\n",
    "            # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "            #                                 method, f\"{method} of {name}\", continuous=True, n_components=2)\n",
    "            plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                          method, f\"{method} of {name}, perplexity={p}\", tsne_perplexity=p, continuous=False, n_components=2, save_path=save_dir + f\"{name}_{method}_p{p}\")\n",
    "            \n",
    "            # 3D plots\n",
    "            # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "            #                                 method, f\"{method} of {name}\", continuous=True, n_components=3)\n",
    "            # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "            #                               method, f\"{method} of {name}, perplexity={p}\", tsne_perplexity=p, continuous=False, n_components=3, save_path=save_dir + f\"{name}_{method}_p{p}\")\n",
    "        else:\n",
    "            print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction methods to each DataFrame\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "method = 'UMAP'\n",
    "# n_neighbors = [2, 5, 10, 20, 50, 100, 200]\n",
    "n_neighbors = [200]\n",
    "min_dist = [0.1, 0.25, 0.5, 0.8, 0.99]\n",
    "save_dir = \"/home/jen-hungwang/Desktop/umap/n200/\"\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    for n in n_neighbors:\n",
    "        for m in min_dist:\n",
    "            if metadata_column in data['df'].columns:\n",
    "                print(f\"Hyperparameters: n_neighbors={n}, min_dist={m}\")\n",
    "                # 2D plots\n",
    "                # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                #                                 method, f\"{method} of {name}\", continuous=True, n_components=2)\n",
    "                plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                              method, f\"{method} of {name}, n_neighbors={n}, min_dist={m}\", n_neighbors=n, min_dist=m, continuous=False, n_components=2, save_path=save_dir + f\"{name}_{method}_n{n}_m{m}\")\n",
    "                \n",
    "                # 3D plots\n",
    "                # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                #                                 method, f\"{method} of {name}\", continuous=True, n_components=3)\n",
    "                # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                #                               method, f\"{method} of {name}, n_neighbors={n}, min_dist={m}\", n_neighbors=n, min_dist=m, continuous=False, n_components=3, save_path=save_dir + f\"{name}_{method}_n{n}_m{m}\")\n",
    "            else:\n",
    "                print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(X_reduced, df, metadata_column, method_name, reduction_method, n_clusters=3, eps=0.5, min_samples=5, covariance='full', save_path=None):\n",
    "    if method_name == 'KMeans':\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    elif method_name == 'DBSCAN':\n",
    "        clusterer = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    elif method_name == 'Agglomerative':\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    elif method_name == 'GaussianMixture':\n",
    "        clusterer = GaussianMixture(n_components=n_clusters, random_state=42, covariance_type=covariance)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported clustering method\")\n",
    "    \n",
    "    if df is None:\n",
    "        df = X_reduced\n",
    "\n",
    "    # Fit the clustering model\n",
    "    labels = clusterer.fit_predict(df)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    metrics = {}\n",
    "    if len(np.unique(labels)) > 1:  # Metrics require at least 2 clusters\n",
    "        metrics['Silhouette'] = silhouette_score(X_reduced, labels) if len(np.unique(labels)) < X_reduced.shape[0] else -1\n",
    "        if method_name == 'GaussianMixture':\n",
    "            metrics['BIC'] = clusterer.bic(X_reduced)\n",
    "        else:\n",
    "            metrics['BIC'] = np.nan\n",
    "    else:\n",
    "        metrics['Silhouette'] = np.nan\n",
    "        metrics['BIC'] = np.nan\n",
    "    # Initialize figure\n",
    "    fig = go.Figure()\n",
    "    unique_labels = np.unique(labels)\n",
    "    color_map = {label: f'rgb({r*255:.0f}, {g*255:.0f}, {b*255:.0f})' for label, (r, g, b) in zip(unique_labels, sns.color_palette('tab10', len(unique_labels)))}\n",
    "    \n",
    "    # Create metrics text for display\n",
    "    metrics_text = (f\"Silhouette: {metrics['Silhouette']:.3f}<br>\"\n",
    "                    f\"BIC: {metrics['BIC']:.3f}\")\n",
    "    \n",
    "    # Check dimensionality of X_reduced for 2D or 3D visualization\n",
    "    if X_reduced.shape[1] >= 3:\n",
    "        # 3D Visualization\n",
    "        for label in unique_labels:\n",
    "            mask = labels == label\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=X_reduced[mask, 0], \n",
    "                y=X_reduced[mask, 1], \n",
    "                z=X_reduced[mask, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(size=3, color=color_map[label], opacity=0.8),\n",
    "                name=f'Cluster {label}' if label != -1 else 'Noise',\n",
    "                showlegend=True\n",
    "            ))\n",
    "        \n",
    "        # Set axis labels based on reduction method for 3D\n",
    "        if reduction_method == 'PCA':\n",
    "            x_label, y_label, z_label = 'PC1', 'PC2', 'PC3'\n",
    "        elif reduction_method == 'LDA':\n",
    "            x_label, y_label, z_label = 'LD1', 'LD2', 'LD3'\n",
    "        elif reduction_method == 't-SNE':\n",
    "            x_label, y_label, z_label = 't-SNE 1', 't-SNE 2', 't-SNE 3'\n",
    "        elif reduction_method == 'UMAP':\n",
    "            x_label, y_label, z_label = 'UMAP 1', 'UMAP 2', 'UMAP 3'\n",
    "        else:\n",
    "            x_label, y_label, z_label = 'Component 1', 'Component 2', 'Component 3'\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'{method_name} Clustering on {reduction_method} (3D, {len(unique_labels)} clusters)',\n",
    "            scene=dict(\n",
    "                xaxis_title=x_label,\n",
    "                yaxis_title=y_label,\n",
    "                zaxis_title=z_label\n",
    "            ),\n",
    "            showlegend=True,\n",
    "            legend=dict(x=0.5, y=0, xanchor=\"center\", yanchor=\"bottom\", orientation=\"h\"),\n",
    "            annotations=[\n",
    "                dict(\n",
    "                    x=0.95,\n",
    "                    y=0.95,\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"paper\",\n",
    "                    text=metrics_text,\n",
    "                    showarrow=False,\n",
    "                    align=\"right\",\n",
    "                    font=dict(size=12),\n",
    "                    bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "                    bordercolor=\"black\",\n",
    "                    borderwidth=1\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        # plot_filename = f'{method_name}_{reduction_method}_3D_clustering.png' if save_path is None else f'{save_path}.png'\n",
    "    \n",
    "    else:\n",
    "        # 2D Visualization\n",
    "        for label in unique_labels:\n",
    "            mask = labels == label\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X_reduced[mask, 0], \n",
    "                y=X_reduced[mask, 1], \n",
    "                mode='markers',\n",
    "                marker=dict(color=color_map[label], size=3),\n",
    "                name=f'Cluster {label}' if label != -1 else 'Noise',\n",
    "                showlegend=True\n",
    "            ))\n",
    "        \n",
    "        # Set axis labels based on reduction method for 2D\n",
    "        if reduction_method == 'PCA':\n",
    "            x_label, y_label = 'PC1', 'PC2'\n",
    "        elif reduction_method == 'LDA':\n",
    "            x_label, y_label = 'LD1', 'LD2'\n",
    "        elif reduction_method == 't-SNE':\n",
    "            x_label, y_label = 't-SNE 1', 't-SNE 2'\n",
    "        elif reduction_method == 'UMAP':\n",
    "            x_label, y_label = 'UMAP 1', 'UMAP 2'\n",
    "        else:\n",
    "            x_label, y_label = 'Component 1', 'Component 2'\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'{method_name} Clustering of {csv_data} on {reduction_method} (2D, {len(unique_labels)} clusters)',\n",
    "            xaxis_title=x_label,\n",
    "            yaxis_title=y_label,\n",
    "            showlegend=True,\n",
    "            legend=dict(x=0.5, y=0, xanchor=\"center\", yanchor=\"bottom\", orientation=\"h\"),\n",
    "            annotations=[\n",
    "                dict(\n",
    "                    x=0.95,\n",
    "                    y=0.95,\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"paper\",\n",
    "                    text=metrics_text,\n",
    "                    showarrow=False,\n",
    "                    align=\"right\",\n",
    "                    font=dict(size=12),\n",
    "                    bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "                    bordercolor=\"black\",\n",
    "                    borderwidth=1\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        # plot_filename = f'{method_name}_{reduction_method}_2D_clustering.png' if save_path is None else f'{save_path}.png'\n",
    "    \n",
    "    # Save the plot as PNG if save_png is True\n",
    "    if save_path:\n",
    "        fig.write_image(f\"{save_path}.png\", width=1200, height=800)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return labels, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cluster_concentration_distribution(df, metadata_column, labels, cluster_method, reduction_method, dim, save_path=None):\n",
    "    # Extract unique clusters (excluding noise if present, e.g., -1 in DBSCAN)\n",
    "    unique_clusters = np.unique(labels[labels != -1])\n",
    "    n_clusters = len(unique_clusters)\n",
    "    \n",
    "    # Determine grid layout (e.g., 2 columns, rows adjust based on number of clusters)\n",
    "    cols = min(2, n_clusters)  # Maximum 2 columns for readability\n",
    "    rows = (n_clusters + cols - 1) // cols  # Calculate rows needed\n",
    "    \n",
    "    # Create subplot figure\n",
    "    fig = make_subplots(rows=rows, cols=cols, subplot_titles=[f'Cluster {cluster}' for cluster in unique_clusters])\n",
    "    \n",
    "    # Get unique concentration values and convert to numerical for sorting\n",
    "    concentration_values = df[metadata_column].unique()\n",
    "    # print(\"Raw concentration values:\", concentration_values)\n",
    "    converted_values = np.array([convert_concentration(val) for val in concentration_values])\n",
    "    # print(\"Converted values:\", converted_values)\n",
    "    # Sort indices based on converted values in descending order\n",
    "    sort_indices = np.argsort(converted_values)[::-1]\n",
    "    sorted_concentration_values = concentration_values[sort_indices]\n",
    "    # print(\"Sorted concentration values:\", sorted_concentration_values)\n",
    "    \n",
    "    \n",
    "    # Add bar charts for each cluster\n",
    "    for idx, cluster in enumerate(unique_clusters):\n",
    "        row = (idx // cols) + 1\n",
    "        col = (idx % cols) + 1\n",
    "        \n",
    "        cluster_mask = labels == cluster\n",
    "        cluster_concentrations = df[metadata_column][cluster_mask]\n",
    "        counts = np.zeros(len(sorted_concentration_values), dtype=int)\n",
    "        for i, conc in enumerate(sorted_concentration_values):\n",
    "            counts[i] = np.sum(cluster_concentrations == conc)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=sorted_concentration_values,\n",
    "                y=counts,\n",
    "                marker_color='rgb(31, 78, 121)'\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col\n",
    "        )\n",
    "    \n",
    "    # Update layout with enhanced resolution and detailed title\n",
    "    fig.update_layout(\n",
    "        title_text=f'Concentration Distribution of {csv_data} Across Clusters ({reduction_method}, {cluster_method}, {n_clusters} Clusters, {dim}D)',\n",
    "        height=400 * rows,  # Increased for better resolution\n",
    "        width=800 * cols,   # Increased for better resolution\n",
    "        showlegend=False    # No global legend since each subplot is labeled\n",
    "    )\n",
    "    \n",
    "    # Update axes titles for all subplots\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            fig.update_xaxes(title_text='Concentration Levels', row=i+1, col=j+1)\n",
    "            fig.update_yaxes(title_text='Count', row=i+1, col=j+1)\n",
    "    \n",
    "    # Save the plot as PNG if save_path is provided\n",
    "    if save_path is not None:\n",
    "        fig.write_image(f\"{save_path}/{name}_{reduction_method}_{cluster_method}_{dim}D_{n_clusters}clusters_histogram.png\", width=800 * cols, height=400 * rows)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clustering to reduced data from PCA, LDA, t-SNE, and UMAP\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "# clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative', 'GaussianMixture']\n",
    "clustering_methods = ['GaussianMixture']\n",
    "# clustering_methods = ['GaussianMixture']\n",
    "reduction_methods = ['LDA']\n",
    "# reduction_methods = ['PCA', 'LDA', 't-SNE', 'UMAP']\n",
    "n_components = 2\n",
    "n_clusters = 3\n",
    "covariance = 'tied'  # 'full', 'tied', 'diag', 'spherical'\n",
    "# save_dir = \"/home/jen-hungwang/Desktop/mnp_analysis/eval/\"\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        \n",
    "        for reduction_method in reduction_methods:\n",
    "            # Compute reduced data based on the method\n",
    "            if reduction_method == 'PCA':\n",
    "                reducer = PCA(n_components=n_components)\n",
    "                X_reduced = reducer.fit_transform(X_scaled)\n",
    "            elif reduction_method == 'LDA':\n",
    "                labels = data['df'][metadata_column].astype(str)\n",
    "                reducer = LDA(n_components=n_components)\n",
    "                X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "            elif reduction_method == 't-SNE':\n",
    "                reducer = TSNE(n_components=n_components, perplexity=30, learning_rate=200, random_state=42)\n",
    "                X_reduced = reducer.fit_transform(X_scaled)\n",
    "            elif reduction_method == 'UMAP':\n",
    "                reducer = umap.UMAP(n_components=n_components, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "                X_reduced = reducer.fit_transform(X_scaled)\n",
    "            \n",
    "            for cluster_method in clustering_methods:\n",
    "                if cluster_method == 'KMeans' or cluster_method == 'Agglomerative' or cluster_method == 'GaussianMixture':\n",
    "                    labels, fig_reduced = perform_clustering(X_reduced, None, metadata_column, cluster_method, reduction_method, n_clusters=n_clusters, covariance=covariance, save_path=save_dir + f\"{csv_data}_{reduction_method}_{cluster_method}_{n_components}D_{n_clusters}clusters\")\n",
    "                    # labels, fig_reduced = perform_clustering(X_reduced, None, metadata_column, cluster_method, reduction_method, n_clusters=n_clusters)\n",
    "                    # labels, fig_all = perform_clustering(X_reduced, X_scaled, metadata_column, cluster_method, reduction_method, n_clusters=n_clusters, save_path=save_dir + f\"{csv_data}_{reduction_method}_{cluster_method}_{n_components}D_{n_clusters}clusters_noreduced\")\n",
    "                elif cluster_method == 'DBSCAN':\n",
    "                    labels = perform_clustering(X_reduced, None, metadata_column, cluster_method, reduction_method, eps=0.5, min_samples=5, save_path=save_dir + f\"{csv_data}_{reduction_method}_{cluster_method}_{n_components}D\")\n",
    "                # print(f\"{cluster_method} clustering labels on {reduction_method} for {name}: {np.unique(labels)}\")\n",
    "                \n",
    "                # Visualize concentration distribution across clusters\n",
    "                # visualize_cluster_concentration_distribution(X_reduced, df, metadata_column, labels, save_path=save_dir + f\"{csv_data}_{reduction_method}_{cluster_method}_{n_components}D_{n_clusters}clusters_histogram\")\n",
    "                # Visualize concentration distribution across clusters\n",
    "                visualize_cluster_concentration_distribution(\n",
    "                    df, metadata_column, labels, cluster_method, reduction_method, n_components, save_path=save_dir\n",
    "                )\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Evaluation Metrics Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed LDA Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "use_gmm = True  # Set to True to use GMM instead of KMeans\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "n_comp = 3  # Number of components for LDA\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        labels = data['df'][metadata_column].astype(str)\n",
    "        reducer = LDA(n_components=n_comp)\n",
    "        X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "        continue\n",
    "\n",
    "    for n_clusters in range_n_clusters:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        ax1.set_ylim([0, len(X_reduced) + (n_clusters + 1) * 10])\n",
    "\n",
    "        if use_gmm:\n",
    "            clusterer = GaussianMixture(n_components=n_clusters, random_state=10)\n",
    "            cluster_labels = clusterer.fit_predict(X_reduced)\n",
    "            centers = clusterer.means_\n",
    "            algo_name = \"GMM\"\n",
    "        else:\n",
    "            clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "            cluster_labels = clusterer.fit_predict(X_reduced)\n",
    "            centers = clusterer.cluster_centers_\n",
    "            algo_name = \"KMeans\"\n",
    "\n",
    "        silhouette_avg = silhouette_score(X_reduced, cluster_labels)\n",
    "        print(\n",
    "            f\"For {name}, n_clusters = {n_clusters}, {algo_name} silhouette_score = {silhouette_avg:.4f}\"\n",
    "        )\n",
    "\n",
    "        sample_silhouette_values = silhouette_samples(X_reduced, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(\n",
    "                np.arange(y_lower, y_upper),\n",
    "                0,\n",
    "                ith_cluster_silhouette_values,\n",
    "                facecolor=color,\n",
    "                edgecolor=color,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "            y_lower = y_upper + 10\n",
    "\n",
    "        ax1.set_title(\"Silhouette plot for the various clusters\")\n",
    "        ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(\n",
    "            X_reduced[:, 0], X_reduced[:, 1], marker=\".\", s=30, lw=0,\n",
    "            alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "        )\n",
    "\n",
    "        ax2.scatter(\n",
    "            centers[:, 0], centers[:, 1], marker=\"o\", c=\"white\", alpha=1,\n",
    "            s=200, edgecolor=\"k\"\n",
    "        )\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker=f\"${i}$\", alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "        ax2.set_title(f\"Cluster visualization using {algo_name}\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle(\n",
    "            f\"{csv_data} - Silhouette analysis with {algo_name}, n_clusters = {n_clusters}, LDA = {n_comp}D\",\n",
    "            fontsize=14, fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "        # Save the plot\n",
    "        filename = f\"{algo_name}_{n_clusters}clusters_{n_comp}D_{csv_data}.png\"\n",
    "        plt.savefig(os.path.join(save_dir, filename), bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(fig)  # Close figure to free memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search LDA Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Function to convert concentration values to numerical for sorting\n",
    "def convert_concentration(val):\n",
    "    try:\n",
    "        # Remove any units (e.g., 'g', 'mg', 'ug', 'ng') and convert to float\n",
    "        val_str = str(val).lower()\n",
    "        if val_str == '0':\n",
    "            return 0.0\n",
    "        for unit in ['g', 'mg', 'ug', 'ng']:\n",
    "            if unit in val_str:\n",
    "                num = float(val_str.replace(unit, ''))\n",
    "                if unit == 'mg':\n",
    "                    num /= 1000  # Convert to grams\n",
    "                elif unit == 'ug':\n",
    "                    num /= 1_000_000  # Convert to grams\n",
    "                elif unit == 'ng':\n",
    "                    num /= 1_000_000_000  # Convert to grams\n",
    "                return num\n",
    "        return float(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return float('-inf')  # Non-numeric values go to the right (lowest)\n",
    "\n",
    "range_n_clusters = [3]\n",
    "range_n_components = [2, 3, 4, 5, 6, 7]\n",
    "use_gmm = True\n",
    "gmm_covariance = 'spherical' # 'full', 'tied', 'diag', 'spherical'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        df = data['df'].copy()  # Create a copy to avoid modifying original data\n",
    "        # Standardize '0' values to string '0' for consistency\n",
    "        df[metadata_column] = df[metadata_column].apply(lambda x: '0' if pd.to_numeric(x, errors='coerce') == 0 else str(x))\n",
    "        X_scaled = data['X_scaled']\n",
    "        labels = df[metadata_column].astype(str)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "        continue\n",
    "\n",
    "    for n_comp in range_n_components:\n",
    "        try:\n",
    "            reducer = LDA(n_components=n_comp)\n",
    "            X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error with n_components={n_comp} for {name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for n_clusters in range_n_clusters:\n",
    "            # Create figure with three subplots\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 7), gridspec_kw={'width_ratios': [1, 1, 1.5]})\n",
    "\n",
    "            # Silhouette Plot (ax1)\n",
    "            ax1.set_xlim([-0.1, 1])\n",
    "            ax1.set_ylim([0, len(X_reduced) + (n_clusters + 1) * 10])\n",
    "\n",
    "            if use_gmm:\n",
    "                clusterer = GaussianMixture(n_components=n_clusters, random_state=10, covariance_type=gmm_covariance)\n",
    "                cluster_labels = clusterer.fit_predict(X_reduced)\n",
    "                centers = clusterer.means_\n",
    "                algo_name = \"GMM\"\n",
    "            else:\n",
    "                clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                cluster_labels = clusterer.fit_predict(X_reduced)\n",
    "                centers = clusterer.cluster_centers_\n",
    "                algo_name = \"KMeans\"\n",
    "\n",
    "            silhouette_avg = silhouette_score(X_reduced, cluster_labels)\n",
    "            print(\n",
    "                f\"For {name}, n_components = {n_comp}, n_clusters = {n_clusters}, {algo_name} silhouette_score = {silhouette_avg:.4f}\"\n",
    "            )\n",
    "\n",
    "            sample_silhouette_values = silhouette_samples(X_reduced, cluster_labels)\n",
    "\n",
    "            y_lower = 10\n",
    "            for i in range(n_clusters):\n",
    "                ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "                ith_cluster_silhouette_values.sort()\n",
    "                size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "                y_upper = y_lower + size_cluster_i\n",
    "\n",
    "                color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "                ax1.fill_betweenx(\n",
    "                    np.arange(y_lower, y_upper),\n",
    "                    0,\n",
    "                    ith_cluster_silhouette_values,\n",
    "                    facecolor=color,\n",
    "                    edgecolor=color,\n",
    "                    alpha=0.7,\n",
    "                )\n",
    "\n",
    "                ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "                y_lower = y_upper + 10\n",
    "\n",
    "            ax1.set_title(\"Silhouette Plot\")\n",
    "            ax1.set_xlabel(\"Silhouette Coefficient Values\")\n",
    "            ax1.set_ylabel(\"Cluster Label\")\n",
    "            ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "            ax1.set_yticks([])\n",
    "            ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "            # Cluster Scatter Plot (ax2)\n",
    "            colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "            ax2.scatter(\n",
    "                X_reduced[:, 0], X_reduced[:, 1], marker=\".\", s=30, lw=0,\n",
    "                alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "            )\n",
    "\n",
    "            ax2.scatter(\n",
    "                centers[:, 0], centers[:, 1], marker=\"o\", c=\"white\", alpha=1,\n",
    "                s=200, edgecolor=\"k\"\n",
    "            )\n",
    "\n",
    "            for i, c in enumerate(centers):\n",
    "                ax2.scatter(c[0], c[1], marker=f\"${i}$\", alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "            ax2.set_title(f\"Cluster Visualization ({algo_name})\")\n",
    "            ax2.set_xlabel(\"Feature Space (1st Dimension)\")\n",
    "            ax2.set_ylabel(\"Feature Space (2nd Dimension)\")\n",
    "\n",
    "            # Concentration Distribution Plot (ax3)\n",
    "            unique_clusters = np.unique(cluster_labels[cluster_labels != -1])\n",
    "            n_clusters = len(unique_clusters)\n",
    "\n",
    "            # Get unique concentration values and sort numerically (highest to lowest)\n",
    "            concentration_values = df[metadata_column].unique()\n",
    "            converted_values = np.array([convert_concentration(val) for val in concentration_values])\n",
    "            # Sort in descending order (highest concentration on left)\n",
    "            sort_indices = np.argsort(converted_values)[::-1]\n",
    "            sorted_concentration_values = concentration_values[sort_indices]\n",
    "\n",
    "            # Create subplots within ax3 for each cluster\n",
    "            for idx, cluster in enumerate(unique_clusters):\n",
    "                cluster_mask = cluster_labels == cluster\n",
    "                cluster_concentrations = df[metadata_column][cluster_mask]\n",
    "                counts = np.zeros(len(sorted_concentration_values), dtype=int)\n",
    "                for i, conc in enumerate(sorted_concentration_values):\n",
    "                    counts[i] = np.sum(cluster_concentrations == conc)\n",
    "\n",
    "                # Plot bar chart for the cluster\n",
    "                ax3.bar(\n",
    "                    np.arange(len(sorted_concentration_values)) + idx * 0.2,  # Offset bars for each cluster\n",
    "                    counts,\n",
    "                    width=0.2,\n",
    "                    label=f'Cluster {cluster}',\n",
    "                    color=cm.nipy_spectral(float(cluster) / n_clusters),\n",
    "                    alpha=0.7\n",
    "                )\n",
    "\n",
    "            ax3.set_title(\"Concentration Distribution Across Clusters\")\n",
    "            ax3.set_xlabel(\"Concentration Levels (High to Low)\")\n",
    "            ax3.set_ylabel(\"Count\")\n",
    "            ax3.set_xticks(np.arange(len(sorted_concentration_values)))\n",
    "            ax3.set_xticklabels(sorted_concentration_values, rotation=45, ha='right')\n",
    "            ax3.legend()\n",
    "\n",
    "            # Overall figure title\n",
    "            plt.suptitle(\n",
    "                f\"{csv_data} - Analysis with {algo_name} (covariance_type={gmm_covariance}), Silhouette = {silhouette_avg:.4f}, n_clusters = {n_clusters}, LDA = {n_comp}D\",\n",
    "                fontsize=14, fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "            # Save the plot±%X\n",
    "            filename = f\"{algo_name}_{n_clusters}clusters_{n_comp}D_{csv_data}_{gmm_covariance}.png\"\n",
    "            plt.savefig(os.path.join(save_dir, filename), bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig)  # Close figure to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Grid Search: GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. LDA Dimension vs. GMM Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "param_grid = {\n",
    "    \"lda__n_components\": range(2, 8),  # Try different LDA dimensions\n",
    "    \"gmm__n_components\": range(2, 8),\n",
    "}\n",
    "\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "eval_method = 'Silhouette' # 'BIC', 'Silhouette'\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column not in data['df'].columns:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "        continue\n",
    "\n",
    "    X_scaled = data['X_scaled']\n",
    "    labels = data['df'][metadata_column].astype(str)\n",
    "\n",
    "    for params in tqdm(ParameterGrid(param_grid), desc=f\"Grid Search in {name}\"):\n",
    "        try:\n",
    "            # Apply LDA with current n_components\n",
    "            n_lda = params['lda__n_components']\n",
    "            reducer = LDA(n_components=n_lda)\n",
    "            X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "\n",
    "            # Fit GMM with current parameters\n",
    "            gmm_params = {\n",
    "                'n_components': params['gmm__n_components'],\n",
    "                'covariance_type': 'tied'\n",
    "            }\n",
    "            gmm = GaussianMixture(**gmm_params)\n",
    "            \n",
    "            if eval_method == 'Silhouette':\n",
    "                cluster_labels = gmm.fit_predict(X_reduced)\n",
    "                if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels):\n",
    "                    score = silhouette_score(X_reduced, cluster_labels)\n",
    "                else:\n",
    "                    score = float('-inf')\n",
    "            elif eval_method == 'BIC':\n",
    "                gmm.fit(X_reduced)\n",
    "                score = gmm_bic_score(gmm, X_reduced)\n",
    "\n",
    "            print(f\"Parameters: {params}, {eval_method} Score: {score}\")\n",
    "\n",
    "            results.append((params, score))\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {name} with params {params}: {e}\")\n",
    "\n",
    "# Find best parameters globally or per dataset\n",
    "best_result = max(results, key=lambda x: x[1])\n",
    "print(\"Best overall:\")\n",
    "print(\"Parameters:\", best_result[0])\n",
    "print(f\"Best {eval_method} score:\", best_result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_lda = pd.DataFrame([\n",
    "    {\n",
    "        \"param_n_components\": params[\"lda__n_components\"],\n",
    "        \"param_n_clusters\": params[\"gmm__n_components\"],\n",
    "        \"mean_test_score\": score\n",
    "    }\n",
    "    for params, score in results\n",
    "])\n",
    "\n",
    "if eval_method == 'BIC':\n",
    "    # Convert negative BIC to positive BIC (since gmm_bic_score returns negative BIC)\n",
    "    df_lda[\"mean_test_score\"] = -df_lda[\"mean_test_score\"]\n",
    "\n",
    "# Rename columns to match desired output\n",
    "df_lda = df_lda.rename(\n",
    "    columns={\n",
    "        \"param_n_components\": \"Number of components (LDA)\",\n",
    "        \"param_n_clusters\": \"Number of clusters (GMM)\",\n",
    "        \"mean_test_score\": f\"{eval_method} score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort by BIC score and display top 5\n",
    "df_lda.sort_values(by=f\"{eval_method} score\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=df_lda,\n",
    "    kind=\"bar\",\n",
    "    x=\"Number of components (LDA)\",\n",
    "    y=f\"{eval_method} score\",\n",
    "    hue=\"Number of clusters (GMM)\",\n",
    ")\n",
    "\n",
    "# g.ax.set_ylim(40, 45)  # Set y-axis limit to 26,000 as requested\n",
    "g.ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)  # Add a grid for better readability\n",
    "\n",
    "# Add title with margin below\n",
    "plt.title(f\"LDA Number of Components vs. GMM Number of Clusters on {csv_data}\", pad=30)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f\"{save_dir}/{csv_data}_lda_dimension_{eval_method}_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GMM Clusters vs. GMM Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_components\": range(2, 3), # (2,8)\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "n_comp = 3  # Number of components for LDA\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        labels = data['df'][metadata_column].astype(str)\n",
    "        reducer = LDA(n_components=n_comp)\n",
    "        X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over parameter combinations with a progress bar\n",
    "for params in tqdm(ParameterGrid(param_grid), desc=\"Grid Search Progress\"):\n",
    "    gmm = GaussianMixture(**params)\n",
    "    gmm.fit(X_reduced)\n",
    "    score = gmm_bic_score(gmm, X_reduced)  # Compute BIC on the full dataset\n",
    "    print(f\"Parameters: {params}, Score (negative BIC): {score}\")\n",
    "    results.append((params, score))\n",
    "\n",
    "# Find the best parameters\n",
    "best_result = max(results, key=lambda x: x[1])\n",
    "best_params = best_result[0]\n",
    "best_score = best_result[1]\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score (negative BIC):\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_bic = pd.DataFrame([\n",
    "    {\n",
    "        \"param_n_components\": params[\"n_components\"],\n",
    "        \"param_covariance_type\": params[\"covariance_type\"],\n",
    "        \"mean_test_score\": -abs(score)\n",
    "    }\n",
    "    for params, score in results\n",
    "])\n",
    "\n",
    "# Convert negative BIC to positive BIC (since gmm_bic_score returns negative BIC)\n",
    "df_bic[\"mean_test_score\"] = -df_bic[\"mean_test_score\"]\n",
    "\n",
    "# Rename columns to match desired output\n",
    "df_bic = df_bic.rename(\n",
    "    columns={\n",
    "        \"param_n_components\": \"Number of clusters\",\n",
    "        \"param_covariance_type\": \"Type of covariance\",\n",
    "        \"mean_test_score\": \"BIC score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort by BIC score and display top 5\n",
    "df_bic.sort_values(by=\"BIC score\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=df_bic,\n",
    "    kind=\"bar\",\n",
    "    x=\"Number of clusters\",\n",
    "    y=\"BIC score\",\n",
    "    hue=\"Type of covariance\",\n",
    "    \n",
    ")\n",
    "\n",
    "# g.ax.set_ylim(250000, 285000)  # Set y-axis limit to 26,000 as requested\n",
    "# g.ax.set_ylim(400000, 450000)\n",
    "g.ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)  # Add a grid for better readability\n",
    "\n",
    "# Add title with margin below\n",
    "plt.title(f\"GMM Number of Clusters by GMM Covariance Types on {csv_data}\", pad=30)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f\"{save_dir}/{csv_data}_bic_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "param_grid = {\n",
    "    \"n_components\": range(2, 8),  # Silhouette score requires at least 2 clusters\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "n_comp = 3  # Number of components for LDA\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        labels = data['df'][metadata_column].astype(str)\n",
    "        reducer = LDA(n_components=n_comp)\n",
    "        X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in tqdm(ParameterGrid(param_grid), desc=\"Grid Search Progress\"):\n",
    "    gmm = GaussianMixture(**params, random_state=42)\n",
    "    cluster_labels = gmm.fit_predict(X_reduced)\n",
    "    # Silhouette score is only valid if there is more than 1 cluster\n",
    "    if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels):\n",
    "        score = silhouette_score(X_reduced, cluster_labels)\n",
    "    else:\n",
    "        score = float('-inf')\n",
    "    print(f\"Parameters: {params}, Silhouette Score: {score}\")\n",
    "    results.append((params, score))\n",
    "\n",
    "# Find the best parameters\n",
    "best_result = max(results, key=lambda x: x[1])\n",
    "best_params = best_result[0]\n",
    "best_score = best_result[1]\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best Silhouette Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_silhouette = pd.DataFrame([\n",
    "    {\n",
    "        \"param_n_components\": params[\"n_components\"],\n",
    "        \"param_covariance_type\": params[\"covariance_type\"],\n",
    "        \"mean_test_score\": score\n",
    "    }\n",
    "    for params, score in results\n",
    "])\n",
    "\n",
    "# Rename columns to match desired output\n",
    "df_silhouette = df_silhouette.rename(\n",
    "    columns={\n",
    "        \"param_n_components\": \"Number of clusters\",\n",
    "        \"param_covariance_type\": \"Type of covariance\",\n",
    "        \"mean_test_score\": \"Silhouette score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort by BIC score and display top 5\n",
    "df_silhouette.sort_values(by=\"Silhouette score\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=df_silhouette,\n",
    "    kind=\"bar\",\n",
    "    x=\"Number of clusters\",\n",
    "    y=\"Silhouette score\",\n",
    "    hue=\"Type of covariance\",\n",
    ")\n",
    "\n",
    "# g.ax.set_ylim(250000, 285000)  # Set y-axis limit to 26,000 as requested\n",
    "# g.ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)  # Add a grid for better readability\n",
    "\n",
    "# Add title with margin below\n",
    "plt.title(f\"GMM Number of Clusters vs. GMM Covariance Types on {csv_data}\", pad=30)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f\"{save_dir}/{csv_data}_silhouette_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LDA Dimension vs. GMM Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define parameter grid for GMM (only covariance_type)\n",
    "gmm_param_grid = {\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "\n",
    "# Metadata column for LDA supervision\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "gmm_n_components = 3  # Fixed number of components for GMM\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Process each dataset in preprocessed_data\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        labels = data['df'][metadata_column].astype(str)\n",
    "        \n",
    "        # Determine max n_components for LDA (min of n_classes-1, n_features)\n",
    "        n_classes = len(np.unique(labels))\n",
    "        n_features = X_scaled.shape[1]\n",
    "        max_n_components = min(n_classes - 1, n_features)\n",
    "        print(\"Max n_components for LDA:\", max_n_components)\n",
    "        \n",
    "        if max_n_components < 1:\n",
    "            print(f\"Warning: Insufficient classes or features for LDA in {csv_data}.\")\n",
    "            continue\n",
    "            \n",
    "        # Define parameter grid for LDA\n",
    "        lda_param_grid = {\n",
    "            \"n_components\": range(1, max_n_components + 1)\n",
    "        }\n",
    "        \n",
    "        # Grid search over LDA parameters\n",
    "        for lda_params in ParameterGrid(lda_param_grid):\n",
    "            # Apply LDA\n",
    "            reducer = LinearDiscriminantAnalysis(**lda_params)\n",
    "            try:\n",
    "                X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "                \n",
    "                # Grid search over GMM covariance_type\n",
    "                for gmm_params in tqdm(ParameterGrid(gmm_param_grid), desc=f\"LDA n_components={lda_params['n_components']}: GMM Grid Search\"):\n",
    "                    gmm = GaussianMixture(n_components=gmm_n_components, **gmm_params, random_state=42)\n",
    "                    cluster_labels = gmm.fit_predict(X_reduced)\n",
    "                    \n",
    "                    # Compute silhouette score (valid if >1 cluster, no noise labels)\n",
    "                    if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels):\n",
    "                        score = silhouette_score(X_reduced, cluster_labels)\n",
    "                    else:\n",
    "                        score = float('-inf')\n",
    "                        \n",
    "                    print(f\"Dataset: {csv_data}, LDA Parameters: {lda_params}, GMM Parameters: {gmm_params}, Silhouette Score: {score}\")\n",
    "                    results.append({\n",
    "                        \"dataset\": csv_data,\n",
    "                        \"lda_params\": lda_params,\n",
    "                        \"gmm_params\": gmm_params,\n",
    "                        \"score\": score\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error for dataset {csv_data}, LDA params={lda_params}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {csv_data}.\")\n",
    "\n",
    "# Find best result\n",
    "if results:\n",
    "    best_result = max(results, key=lambda x: x[\"score\"])\n",
    "    print(f\"Best Dataset: {best_result['dataset']}\")\n",
    "    print(f\"Best LDA Parameters: {best_result['lda_params']}\")\n",
    "    print(f\"Best GMM Parameters: {best_result['gmm_params']}\")\n",
    "    print(f\"Best Silhouette Score: {best_result['score']}\")\n",
    "else:\n",
    "    print(\"No valid results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_silhouette = pd.DataFrame([\n",
    "    {\n",
    "        \"dataset\": result[\"dataset\"],\n",
    "        \"param_lda_n_components\": result[\"lda_params\"][\"n_components\"],\n",
    "        \"param_gmm_covariance_type\": result[\"gmm_params\"][\"covariance_type\"],\n",
    "        \"mean_test_score\": result[\"score\"]\n",
    "    }\n",
    "    for result in results\n",
    "])\n",
    "\n",
    "# Rename columns to match desired output\n",
    "df_silhouette = df_silhouette.rename(\n",
    "    columns={\n",
    "        \"param_lda_n_components\": \"Number of Components (LDA)\",\n",
    "        \"param_gmm_covariance_type\": \"Type of Covariance (GMM)\",\n",
    "        \"mean_test_score\": \"Silhouette score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort by silhouette score (descending) and display top 5\n",
    "df_silhouette.sort_values(by=\"Silhouette score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=df_silhouette,\n",
    "    kind=\"bar\",\n",
    "    x=\"Number of Components (LDA)\",\n",
    "    y=\"Silhouette score\",\n",
    "    hue=\"Type of Covariance (GMM)\",\n",
    ")\n",
    "\n",
    "# g.ax.set_ylim(250000, 285000)  # Set y-axis limit to 26,000 as requested\n",
    "# g.ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)  # Add a grid for better readability\n",
    "\n",
    "# Add title with margin below\n",
    "plt.title(f\"LDA Number of Components vs. GMM Covariance Types ({gmm_n_components} Clusters) on {csv_data}\", pad=30)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f\"{save_dir}/{csv_data}_LDA_DimvsGMM_covariance_silhouette_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. No LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_components\": range(2, 7),\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "# n_comp = 3  # Number of components for LDA\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        # labels = data['df'][metadata_column].astype(str)\n",
    "        # reducer = LDA(n_components=n_comp)\n",
    "        # X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over parameter combinations with a progress bar\n",
    "for params in tqdm(ParameterGrid(param_grid), desc=\"Grid Search Progress\"):\n",
    "    gmm = GaussianMixture(**params)\n",
    "    gmm.fit(X_scaled)\n",
    "    score = gmm_bic_score(gmm, X_scaled)  # Compute BIC on the full dataset\n",
    "    print(f\"Parameters: {params}, Score (negative BIC): {score}\")\n",
    "    results.append((params, score))\n",
    "\n",
    "# Find the best parameters\n",
    "best_result = max(results, key=lambda x: x[1])\n",
    "best_params = best_result[0]\n",
    "best_score = best_result[1]\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score (negative BIC):\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Grid Search: Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clustering to reduced data from PCA, LDA, t-SNE, and UMAP\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative', 'GaussianMixture']\n",
    "reduction_methods = ['LDA']\n",
    "# reduction_methods = ['PCA', 'LDA', 't-SNE', 'UMAP']\n",
    "n_components_list = [2, 3]\n",
    "n_clusters_list = [3, 4, 5, 6, 7, 8]\n",
    "save_dir = \"/home/jen-hungwang/Desktop/eval/\"\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "\n",
    "        for reduction_method in reduction_methods:\n",
    "            for n_components in n_components_list:\n",
    "                # Compute reduced data based on the method\n",
    "                if reduction_method == 'PCA':\n",
    "                    reducer = PCA(n_components=n_components)\n",
    "                    X_reduced = reducer.fit_transform(X_scaled)\n",
    "                elif reduction_method == 'LDA':\n",
    "                    labels = data['df'][metadata_column].astype(str)\n",
    "                    reducer = LDA(n_components=n_components)\n",
    "                    X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "                elif reduction_method == 't-SNE':\n",
    "                    reducer = TSNE(n_components=n_components, perplexity=30, learning_rate=200, random_state=42)\n",
    "                    X_reduced = reducer.fit_transform(X_scaled)\n",
    "                elif reduction_method == 'UMAP':\n",
    "                    reducer = umap.UMAP(n_components=n_components, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "                    X_reduced = reducer.fit_transform(X_scaled)\n",
    "\n",
    "                for cluster_method in clustering_methods:\n",
    "                    if cluster_method in ['KMeans', 'Agglomerative', 'GaussianMixture']:\n",
    "                        for n_clusters in n_clusters_list:\n",
    "                            labels = perform_clustering(\n",
    "                                X_reduced, \n",
    "                                None, \n",
    "                                metadata_column, \n",
    "                                cluster_method, \n",
    "                                reduction_method, \n",
    "                                n_clusters=n_clusters, \n",
    "                                save_path=save_dir + f\"{name}_{reduction_method}_{cluster_method}_{n_components}D_{n_clusters}clusters\"\n",
    "                            )\n",
    "                            print(f\"{cluster_method} clustering labels on {reduction_method} for {name} ({n_components}D, {n_clusters} clusters): {np.unique(labels)}\")\n",
    "                    elif cluster_method == 'DBSCAN':\n",
    "                        labels = perform_clustering(\n",
    "                            X_reduced, \n",
    "                            None, \n",
    "                            metadata_column, \n",
    "                            cluster_method, \n",
    "                            reduction_method, \n",
    "                            eps=0.5, \n",
    "                            min_samples=5, \n",
    "                            save_path=save_dir + f\"{name}_{reduction_method}_{cluster_method}_{n_components}D\"\n",
    "                        )\n",
    "                        print(f\"{cluster_method} clustering labels on {reduction_method} for {name} ({n_components}D): {np.unique(labels)}\")\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "figs       = [fig_original, fig_reduced]\n",
    "fig_labels = ['Original classes',\n",
    "              'Cluster with dimensionality reduction']\n",
    "num_plots = 2\n",
    "\n",
    "# -- 1. build a 3-column subplot skeleton, each cell is 3-D --------------\n",
    "combined_fig = make_subplots(\n",
    "    rows=1, cols=num_plots,\n",
    "    specs=[[{\"type\": \"scene\"}]*num_plots],               # 3× Scatter3d panels\n",
    "    subplot_titles=fig_labels,\n",
    "    horizontal_spacing=0.07                      # little gap between plots\n",
    ")\n",
    "\n",
    "# -- 2. copy every trace into the right cell, tweak the marker size -------\n",
    "for col, (fig, label) in enumerate(zip(figs, fig_labels), start=1):\n",
    "    for trace in fig.data:\n",
    "        # make dots smaller (overwrite whatever size was there)\n",
    "        if hasattr(trace, \"marker\"):          # safety check\n",
    "            trace.marker.size = 3\n",
    "        # prefix trace names with panel label so the legend is explicit\n",
    "        trace.name = f\"{trace.name}\"\n",
    "        combined_fig.add_trace(trace, row=1, col=col)\n",
    "\n",
    "# -- 3. carry over each panel’s axis titles -------------------------------\n",
    "for col, fig in enumerate(figs, start=1):\n",
    "    if hasattr(fig.layout, \"scene\"):  # 3-D source figure\n",
    "        tgt_scene = \"scene\" if col == 1 else f\"scene{col}\"\n",
    "        combined_fig.update_layout({\n",
    "            tgt_scene: dict(\n",
    "                xaxis_title = fig.layout.scene.xaxis.title.text,\n",
    "                yaxis_title = fig.layout.scene.yaxis.title.text,\n",
    "                zaxis_title = fig.layout.scene.zaxis.title.text\n",
    "            )\n",
    "        })\n",
    "    else:                             # 2-D source figure (just in case)\n",
    "        combined_fig.update_xaxes(title_text=fig.layout.xaxis.title.text,\n",
    "                                  row=1, col=col)\n",
    "        combined_fig.update_yaxes(title_text=fig.layout.yaxis.title.text,\n",
    "                                  row=1, col=col)\n",
    "\n",
    "# -- 4. overall figure cosmetics ------------------------------------------\n",
    "combined_fig.update_layout(\n",
    "    title=\"Combined clustering results\",\n",
    "    height=500, width=1350,\n",
    "    legend=dict(itemsizing=\"constant\")   # keep legend entry size compact\n",
    ")\n",
    "\n",
    "combined_fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function for clustering results\n",
    "def evaluate_clustering(X_reduced, labels, df, metadata_column):\n",
    "    # Convert concentration to categorical labels as ground truth\n",
    "    true_labels = df[metadata_column].astype(str)\n",
    "    \n",
    "    # Silhouette Score\n",
    "    silhouette_avg = silhouette_score(X_reduced, labels) if len(np.unique(labels)) > 1 else None\n",
    "    \n",
    "    # Adjusted Rand Score (requires true labels)\n",
    "    adjusted_rand = adjusted_rand_score(true_labels, labels) if len(np.unique(labels)) > 1 else None\n",
    "    \n",
    "    # Calinski-Harabasz Score\n",
    "    ch_score = calinski_harabasz_score(X_reduced, labels) if len(np.unique(labels)) > 1 else None\n",
    "    \n",
    "    print(f\"Evaluation Metrics for Clustering:\")\n",
    "    print(f\"Silhouette Score: {silhouette_avg:.4f}\" if silhouette_avg is not None else \"Silhouette Score: N/A (single cluster)\")\n",
    "    print(f\"Adjusted Rand Score: {adjusted_rand:.4f}\" if adjusted_rand is not None else \"Adjusted Rand Score: N/A (single cluster or no true labels)\")\n",
    "    print(f\"Calinski-Harabasz Score: {ch_score:.4f}\" if ch_score is not None else \"Calinski-Harabasz Score: N/A (single cluster)\")\n",
    "    return silhouette_avg, adjusted_rand, ch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering results for each reduction method, clustering method, and dataset\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative']\n",
    "reduction_methods = ['LDA']\n",
    "# reduction_methods = ['PCA', 'LDA', 't-SNE', 'UMAP']\n",
    "n_components = 3\n",
    "save_dir = \"/home/jen-hungwang/Desktop/eval/\"\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        \n",
    "        for reduction_method in reduction_methods:\n",
    "            # Compute reduced data based on the method\n",
    "            if reduction_method == 'PCA':\n",
    "                reducer = PCA(n_components=n_components)\n",
    "                X_reduced = reducer.fit_transform(X_scaled)\n",
    "            elif reduction_method == 'LDA':\n",
    "                labels = data['df'][metadata_column].astype(str)\n",
    "                reducer = LDA(n_components=n_components)\n",
    "                X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "            elif reduction_method == 't-SNE':\n",
    "                reducer = TSNE(n_components=n_components, perplexity=30, learning_rate=200, random_state=42)\n",
    "                X_reduced = reducer.fit_transform(X_scaled)\n",
    "            elif reduction_method == 'UMAP':\n",
    "                reducer = umap.UMAP(n_components=n_components, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "                X_reduced = reducer.fit_transform(X_scaled)\n",
    "            \n",
    "            for cluster_method in clustering_methods:\n",
    "                if cluster_method == 'KMeans' or cluster_method == 'Agglomerative':\n",
    "                    labels = perform_clustering(X_reduced, None, metadata_column, cluster_method, reduction_method, n_clusters=3, save_path=save_dir + f\"{name}_{reduction_method}_{cluster_method}\")\n",
    "                elif cluster_method == 'DBSCAN':\n",
    "                    labels = perform_clustering(X_reduced, None, metadata_column, cluster_method, reduction_method, eps=0.5, min_samples=5, save_path=save_dir + f\"{name}_{reduction_method}_{cluster_method}\")\n",
    "                print(f\"\\nEvaluating {cluster_method} clustering on {reduction_method} for {name}:\")\n",
    "                evaluate_clustering(X_reduced, labels, data['df'], metadata_column)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
