{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import re\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/jen-hungwang/Documents/mnp-liver/results/\"\n",
    "# save_dir = \"/storage/jenhung/results/mnp/\" # Adjusted for Docker environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "csv_dir = Path(\"/home/jen-hungwang/Documents/mnp-liver/csv\")  # Adjusted for local environment\n",
    "# csv_dir = Path(\"/storage/jenhung/data/mnp_liver\")  # Adjusted for Docker environment\n",
    "hep_path = csv_dir / \"hep\" # hep or huh\n",
    "csv_data = \"df_SingleCell_AO_HEPG2_110341.csv\" # \"df_HUH7_SingleCell_102912.csv\", \"df_HUH7_SingleCell_110341.csv\", \"df_HUH7_SingleCell_191735.csv\"\n",
    "f = hep_path / csv_data\n",
    "metadata_column = \"Metadata_concentration_perliter\"  # Adjusted for local environment\n",
    "\n",
    "df = pd.read_csv(f, sep=\",\", header=0, dtype={metadata_column: 'string'})\n",
    "# df = pd.read_csv(f, sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly drop NC for data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df[metadata_column].value_counts()\n",
    "num_classes = len(value_counts)\n",
    "print(f\"Unique values and their counts in column '{metadata_column}':\")\n",
    "print(value_counts)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_to_keep = 50\n",
    "\n",
    "# Identify rows where the column is exactly the string '0'\n",
    "zero_mask = df[metadata_column] == '0'\n",
    "\n",
    "num_zero_rows = zero_mask.sum()\n",
    "rows_to_keep = int(num_zero_rows * percentage_to_keep / 100)\n",
    "\n",
    "print(f\"Number of rows with {metadata_column} == '0': {num_zero_rows}\")\n",
    "print(f\"Number of rows to keep ({percentage_to_keep:.2f}% of zeros): {rows_to_keep}\")\n",
    "\n",
    "# Sample rows from '0's only if any exist\n",
    "if rows_to_keep > 0:\n",
    "    zero_indices = df.index[zero_mask]\n",
    "    zero_indices_to_keep = np.random.default_rng(42).choice(zero_indices, size=rows_to_keep, replace=False)\n",
    "    keep_mask = ~zero_mask\n",
    "    keep_mask.loc[zero_indices_to_keep] = True\n",
    "else:\n",
    "    keep_mask = ~zero_mask\n",
    "\n",
    "# Filter and reset index\n",
    "df_filtered = df.loc[keep_mask].reset_index(drop=True)\n",
    "\n",
    "# Check result\n",
    "value_counts = df_filtered[metadata_column].value_counts()\n",
    "print(f\"Unique values and their counts in column '{metadata_column}' after dropping {100 - percentage_to_keep:.2f}% of '0' entries:\")\n",
    "print(value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly delete df to free memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, nan_threshold=0.0001):\n",
    "    # Select feature columns using a generator to avoid list storage\n",
    "    feature_columns = [col for col in df.columns if not (col.startswith(('Metadata_', 'Image_')) or col.endswith('_ObjectNumber'))]\n",
    "    print(f\"Number of feature columns: {len(feature_columns)}\")\n",
    "    \n",
    "    # Convert to float32 upfront to reduce memory (assuming numerical data)\n",
    "    X = df[feature_columns].astype('float32', copy=False)\n",
    "    \n",
    "    # Calculate NaN and Inf counts in one pass\n",
    "    nan_counts = X.isna().sum()\n",
    "    inf_counts = np.isinf(X).sum()\n",
    "    \n",
    "    # Identify columns with NaN or Inf exceeding threshold\n",
    "    threshold = X.shape[0] * nan_threshold\n",
    "    invalid_columns = set(nan_counts[nan_counts >= threshold].index).union(\n",
    "        inf_counts[inf_counts >= threshold].index\n",
    "    )\n",
    "    valid_columns = [col for col in feature_columns if col not in invalid_columns]\n",
    "    \n",
    "    # Print columns with NaN or Inf if any\n",
    "    columns_with_nan_or_inf = set(nan_counts[nan_counts > 0].index).union(\n",
    "        inf_counts[inf_counts > 0].index\n",
    "    )\n",
    "    if columns_with_nan_or_inf:\n",
    "        print(\"\\nColumns with at least one NaN or Inf value:\")\n",
    "        print(f\"{'Column':<60} {'NaN Count':>10} {'Inf Count':>10}\")\n",
    "        print(\"-\" * 80)\n",
    "        for col in sorted(columns_with_nan_or_inf):\n",
    "            print(f\"{col:<60} {nan_counts[col]:>10} {inf_counts[col]:>10}\")\n",
    "        print(f\"Total columns with NaN or Inf: {len(columns_with_nan_or_inf)}\")\n",
    "    \n",
    "    print(f\"Number of valid columns: {len(valid_columns)}\")\n",
    "    if not valid_columns:\n",
    "        raise ValueError(\"No valid columns remain after filtering.\")\n",
    "    \n",
    "    # Select valid columns in-place\n",
    "    X.drop(columns=[col for col in X.columns if col not in valid_columns], inplace=True)\n",
    "    \n",
    "    # Replace inf with NaN in-place\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Fill NaN with median in one pass\n",
    "    medians = X.median()\n",
    "    X.fillna(medians, inplace=True)\n",
    "    \n",
    "    # Check for remaining NaN values\n",
    "    nan_count_after_fill = X.isna().sum().sum()\n",
    "    print(f\"\\nNaN count after filling with median: {nan_count_after_fill}\")\n",
    "    if nan_count_after_fill > 0:\n",
    "        print(\"Warning: Some NaN values remain. Filling with zero.\")\n",
    "        X.fillna(0, inplace=True)\n",
    "    \n",
    "    # Check if data is valid\n",
    "    if X.shape[0] == 0 or X.shape[1] == 0:\n",
    "        raise ValueError(\"No rows/columns remain after preprocessing.\")\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X).astype('float32')  # Keep float32 for scaled data\n",
    "    \n",
    "    # Explicitly delete X to free memory\n",
    "    del X\n",
    "    \n",
    "    return X_scaled, valid_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess DataFrame\n",
    "dataframes = {'df': df_filtered}\n",
    "preprocessed_data = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"Original {name} shape: {df.shape}\")\n",
    "    print(f\"Preprocessing {name}...\")\n",
    "    X_scaled, valid_columns = preprocess_dataframe(df)\n",
    "    preprocessed_data[name] = {'X_scaled': X_scaled, 'valid_columns': valid_columns, 'df': df}\n",
    "    print(f\"Preprocessed {name} with {len(valid_columns)} valid columns.\\n\")\n",
    "    \n",
    "    # Delete df_filtered if not needed later\n",
    "    del df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D & 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import umap\n",
    "import seaborn as sns\n",
    "\n",
    "def convert_concentration(value):\n",
    "    if pd.isna(value) or value == \"\":\n",
    "        return 0.0\n",
    "    try:\n",
    "        value = str(value).lower().replace(\" \", \"\")\n",
    "        match = re.match(r'(\\d*\\.?\\d+)([mnu]?g)?', value)\n",
    "        if match:\n",
    "            num = float(match.group(1))\n",
    "            unit = match.group(2) or ''\n",
    "            if unit == 'mg':\n",
    "                return num * 1e-3\n",
    "            elif unit == 'ug':\n",
    "                return num * 1e-6\n",
    "            elif unit == 'ng':\n",
    "                return num * 1e-9\n",
    "            return num  # 'g' or no unit\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def plot_dimensionality_reduction(X_scaled, df, valid_columns, metadata_column, method_name, title, \n",
    "                                  tsne_perplexity=30, n_neighbors=15, min_dist=0.1, continuous=True, \n",
    "                                  n_components=2, save_path=None):\n",
    "    # Convert X_scaled to float32 to reduce memory\n",
    "    X_scaled = X_scaled.astype('float32', copy=False)\n",
    "    \n",
    "    # Initialize reducer and labels\n",
    "    if method_name == 'PCA':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        x_label, y_label = 'PC1', 'PC2'\n",
    "        z_label = 'PC3' if n_components == 3 else None\n",
    "    elif method_name == 't-SNE':\n",
    "        reducer = TSNE(n_components=n_components, perplexity=tsne_perplexity, learning_rate='auto', random_state=42)\n",
    "        x_label, y_label = 't-SNE 1', 't-SNE 2'\n",
    "        z_label = 't-SNE 3' if n_components == 3 else None\n",
    "    elif method_name == 'UMAP':\n",
    "        reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=42)\n",
    "        x_label, y_label = 'UMAP 1', 'UMAP 2'\n",
    "        z_label = 'UMAP 3' if n_components == 3 else None\n",
    "    elif method_name == 'LDA':\n",
    "        labels = df[metadata_column].astype('string', copy=False)\n",
    "        reducer = LDA(n_components=n_components)\n",
    "        x_label, y_label = 'LD1', 'LD2'\n",
    "        z_label = 'LD3' if n_components == 3 else None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method\")\n",
    "\n",
    "    # Perform dimensionality reduction\n",
    "    try:\n",
    "        X_reduced = (reducer.fit_transform(X_scaled, labels) if method_name == 'LDA' \n",
    "                     else reducer.fit_transform(X_scaled)).astype('float32')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {method_name} reduction: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize figure based on n_components\n",
    "    is_3d = n_components == 3\n",
    "    fig = go.Figure()\n",
    "    title_suffix = ' (Continuous)' if continuous else ' (Categorical)'\n",
    "    \n",
    "    # Plotting\n",
    "    if continuous:\n",
    "        concentrations = df[metadata_column].apply(convert_concentration).astype('float32')\n",
    "        if is_3d:\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=X_reduced[:, 0], y=X_reduced[:, 1], z=X_reduced[:, 2], mode='markers',\n",
    "                marker=dict(color=concentrations, colorscale='Viridis', showscale=True, \n",
    "                           colorbar=dict(title=f'{metadata_column} (g)')))\n",
    "            )\n",
    "        else:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X_reduced[:, 0], y=X_reduced[:, 1], mode='markers',\n",
    "                marker=dict(color=concentrations, colorscale='Viridis', showscale=True, \n",
    "                           colorbar=dict(title=f'{metadata_column} (g)')))\n",
    "            )\n",
    "    else:\n",
    "        labels = df[metadata_column].astype('string', copy=False)\n",
    "        unique_labels = sorted(labels.unique(), key=convert_concentration, reverse=True)\n",
    "        color_map = {label: f'rgb({int(r*255)}, {int(g*255)}, {int(b*255)})' \n",
    "                     for label, (r, g, b) in zip(unique_labels, sns.color_palette('tab10', len(unique_labels)))}\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            mask = labels == label\n",
    "            if is_3d:\n",
    "                fig.add_trace(go.Scatter3d(\n",
    "                    x=X_reduced[mask, 0], y=X_reduced[mask, 1], z=X_reduced[mask, 2], \n",
    "                    mode='markers', marker=dict(color=color_map[label], size=3),\n",
    "                    name=str(label), showlegend=True\n",
    "                ))\n",
    "            else:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=X_reduced[mask, 0], y=X_reduced[mask, 1], \n",
    "                    mode='markers', marker=dict(color=color_map[label], size=3),\n",
    "                    name=str(label), showlegend=True\n",
    "                ))\n",
    "\n",
    "        fig.update_layout(showlegend=True, legend=dict(itemsizing='constant'))\n",
    "\n",
    "    # Update layout\n",
    "    layout = dict(\n",
    "        title=f\"{title}{title_suffix} ({'3D' if is_3d else '2D'}, {len(valid_columns)} features)\",\n",
    "        **({'scene': dict(xaxis_title=x_label, yaxis_title=y_label, zaxis_title=z_label)} if is_3d \n",
    "           else {'xaxis_title': x_label, 'yaxis_title': y_label})\n",
    "    )\n",
    "    fig.update_layout(**layout)\n",
    "\n",
    "    # Save plot if save_path is provided\n",
    "    if save_path:\n",
    "        fig.write_image(f\"{save_path}_{'3D' if is_3d else '2D'}.png\", width=1200, height=800)\n",
    "\n",
    "    # Free memory\n",
    "    del X_reduced\n",
    "    if continuous:\n",
    "        del concentrations\n",
    "    else:\n",
    "        del labels\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction methods to each DataFrame\n",
    "method = 'LDA'\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        # 2D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=2)\n",
    "        plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=2, \n",
    "                                      save_path=save_dir + f\"{csv_data}_{method}\")\n",
    "        \n",
    "        # 3D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=3)\n",
    "        fig_original = plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=3,\n",
    "                                      save_path=save_dir + f\"{csv_data}_{method}\")\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search LDA Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Function to convert concentration values to numerical for sorting\n",
    "def convert_concentration(value):\n",
    "    if pd.isna(value) or value == \"\":\n",
    "        return 0.0\n",
    "    try:\n",
    "        value = str(value).lower().replace(\" \", \"\")  # Normalize input\n",
    "        # Use regex to extract number and unit\n",
    "        match = re.match(r'(\\d*\\.?\\d+)([mnu]?g)?', value)\n",
    "        if match:\n",
    "            num = float(match.group(1))\n",
    "            unit = match.group(2) or ''\n",
    "            if unit == 'mg':\n",
    "                return num * 1e-3  # Convert mg to g\n",
    "            elif unit == 'ug':\n",
    "                return num * 1e-6  # Convert ug to g\n",
    "            elif unit == 'ng':\n",
    "                return num * 1e-9  # Convert ng to g\n",
    "            elif unit == 'g' or not unit:  # Includes pure numbers or 'g'\n",
    "                return num\n",
    "        return float(value)  # Fallback for pure numbers\n",
    "    except ValueError:\n",
    "        return 0.0  # Default to 0 for invalid entries\n",
    "    \n",
    "# Configuration\n",
    "range_n_clusters = [3]\n",
    "range_n_components = [4] # [2, 3, 4, 5, 6, 7]\n",
    "use_gmm = True\n",
    "gmm_covariance_types = ['full', 'tied'] # ['full', 'tied', 'diag', 'spherical']\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column not in data['df'].columns:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "        continue\n",
    "\n",
    "    # Use the DataFrame directly, avoiding copy\n",
    "    df = data['df']\n",
    "  \n",
    "    # Standardize '0' values in-place to string '0' for consistency\n",
    "    df[metadata_column] = df[metadata_column].apply(lambda x: x.lower().strip())\n",
    "\n",
    "    X_scaled = data['X_scaled'].astype('float32', copy=False)  # Ensure float32\n",
    "\n",
    "    for n_comp in range_n_components:\n",
    "        try:\n",
    "            reducer = LDA(n_components=n_comp)\n",
    "            X_reduced = reducer.fit_transform(X_scaled, df[metadata_column])\n",
    "        except ValueError as e:\n",
    "            print(f\"Error with n_components={n_comp} for {name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Iterate over covariance types if using GMM\n",
    "        covariance_iter = gmm_covariance_types if use_gmm else [None]\n",
    "        for gmm_covariance in covariance_iter:\n",
    "            for n_clusters in range_n_clusters:\n",
    "                # Create figure with three subplots\n",
    "                fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 7), gridspec_kw={'width_ratios': [1, 1, 1.5]})\n",
    "\n",
    "                # Clustering\n",
    "                try:\n",
    "                    if use_gmm:\n",
    "                        clusterer = GaussianMixture(\n",
    "                            n_components=n_clusters, random_state=10, covariance_type=gmm_covariance\n",
    "                        )\n",
    "                        algo_name = f\"GMM_{gmm_covariance}\"\n",
    "                    else:\n",
    "                        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                        algo_name = \"KMeans\"\n",
    "\n",
    "                    cluster_labels = clusterer.fit_predict(X_reduced)\n",
    "                    centers = clusterer.means_ if use_gmm else clusterer.cluster_centers_\n",
    "                    centers = centers.astype('float32')  # Reduce memory\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in clustering for {name}, n_components={n_comp}, n_clusters={n_clusters}, {algo_name}: {e}\")\n",
    "                    plt.close(fig)\n",
    "                    continue\n",
    "\n",
    "                silhouette_avg = silhouette_score(X_reduced, cluster_labels)\n",
    "                print(\n",
    "                    f\"For {name}, n_components={n_comp}, n_clusters={n_clusters}, {algo_name}, \"\n",
    "                    f\"silhouette_score={silhouette_avg:.4f}\"\n",
    "                )\n",
    "\n",
    "                # Silhouette Plot (ax1)\n",
    "                ax1.set_xlim([-0.1, 1])\n",
    "                ax1.set_ylim([0, len(X_reduced) + (n_clusters + 1) * 10])\n",
    "                sample_silhouette_values = silhouette_samples(X_reduced, cluster_labels)\n",
    "\n",
    "                y_lower = 10\n",
    "                for i in range(n_clusters):\n",
    "                    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "                    ith_cluster_silhouette_values.sort()\n",
    "                    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "                    y_upper = y_lower + size_cluster_i\n",
    "                    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "                    ax1.fill_betweenx(\n",
    "                        np.arange(y_lower, y_upper),\n",
    "                        0,\n",
    "                        ith_cluster_silhouette_values,\n",
    "                        facecolor=color,\n",
    "                        edgecolor=color,\n",
    "                        alpha=0.7,\n",
    "                    )\n",
    "                    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "                    y_lower = y_upper + 10\n",
    "\n",
    "                ax1.set_title(\"Silhouette Plot\")\n",
    "                ax1.set_xlabel(\"Silhouette Coefficient Values\")\n",
    "                ax1.set_ylabel(\"Cluster Label\")\n",
    "                ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "                ax1.set_yticks([])\n",
    "                ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "                # Cluster Scatter Plot (ax2)\n",
    "                colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "                ax2.scatter(\n",
    "                    X_reduced[:, 0], X_reduced[:, 1], marker=\".\", s=30, lw=0,\n",
    "                    alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "                )\n",
    "                ax2.scatter(\n",
    "                    centers[:, 0], centers[:, 1], marker=\"o\", c=\"white\", alpha=1,\n",
    "                    s=200, edgecolor=\"k\"\n",
    "                )\n",
    "                for i, c in enumerate(centers):\n",
    "                    ax2.scatter(c[0], c[1], marker=f\"${i}$\", alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "                ax2.set_title(f\"Cluster Visualization ({algo_name})\")\n",
    "                ax2.set_xlabel(\"Feature Space (1st Dimension)\")\n",
    "                ax2.set_ylabel(\"Feature Space (2nd Dimension)\")\n",
    "\n",
    "                # Concentration Distribution Plot (ax3)\n",
    "                unique_clusters = np.unique(cluster_labels[cluster_labels != -1])\n",
    "                concentration_values = df[metadata_column].unique()\n",
    "                converted_values = np.array([convert_concentration(val) for val in concentration_values])\n",
    "                sort_indices = np.argsort(converted_values)[::-1]\n",
    "                sorted_concentration_values = concentration_values[sort_indices]\n",
    "\n",
    "                for idx, cluster in enumerate(unique_clusters):\n",
    "                    cluster_mask = cluster_labels == cluster\n",
    "                    cluster_concentrations = df[metadata_column][cluster_mask]\n",
    "                    counts = np.zeros(len(sorted_concentration_values), dtype='int32')  # Use int32\n",
    "                    for i, conc in enumerate(sorted_concentration_values):\n",
    "                        counts[i] = np.sum(cluster_concentrations == conc)\n",
    "\n",
    "                    ax3.bar(\n",
    "                        np.arange(len(sorted_concentration_values)) + idx * 0.2,\n",
    "                        counts,\n",
    "                        width=0.2,\n",
    "                        label=f'Cluster {cluster}',\n",
    "                        color=cm.nipy_spectral(float(cluster) / n_clusters),\n",
    "                        alpha=0.7\n",
    "                    )\n",
    "\n",
    "                ax3.set_title(\"Concentration Distribution Across Clusters\")\n",
    "                ax3.set_xlabel(\"Concentration Levels (High to Low)\")\n",
    "                ax3.set_ylabel(\"Count\")\n",
    "                ax3.set_xticks(np.arange(len(sorted_concentration_values)))\n",
    "                ax3.set_xticklabels(sorted_concentration_values, rotation=45, ha='right')\n",
    "                ax3.legend()\n",
    "\n",
    "                # Overall figure title\n",
    "                plt.suptitle(\n",
    "                    f\"{csv_data} - Analysis with {algo_name}, Silhouette={silhouette_avg:.4f}, \"\n",
    "                    f\"n_clusters={n_clusters}, LDA={n_comp}D\",\n",
    "                    fontsize=14, fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "                # Save and close plot\n",
    "                filename = f\"{algo_name}_{n_clusters}clusters_{n_comp}D_{csv_data}.png\"\n",
    "                plt.savefig(os.path.join(save_dir, filename), bbox_inches='tight', dpi=300)\n",
    "                plt.close(fig)  # Free memory\n",
    "                del fig  # Explicitly delete figure\n",
    "\n",
    "        # Free memory after each n_components iteration\n",
    "        del X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Grid Search: GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "# Configuration\n",
    "param_grid = {\n",
    "    \"lda__n_components\": range(2, 4),\n",
    "    \"gmm__n_components\": range(2, 4),\n",
    "    \"gmm__covariance_type\": ['full', 'tied'] # , 'diag', 'spherical']\n",
    "}\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "eval_method = 'Silhouette'  # 'BIC', 'Silhouette'\n",
    "results = []\n",
    "\n",
    "# Grid search\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column not in data['df'].columns:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "        continue\n",
    "\n",
    "    X_scaled = data['X_scaled'].astype('float32', copy=False)\n",
    "    labels = data['df'][metadata_column].astype('string', copy=False)\n",
    "\n",
    "    for params in tqdm(ParameterGrid(param_grid), desc=f\"Grid Search in {name}\"):\n",
    "        try:\n",
    "            # Apply LDA\n",
    "            n_lda = params['lda__n_components']\n",
    "            reducer = LDA(n_components=n_lda)\n",
    "            X_reduced = reducer.fit_transform(X_scaled, labels).astype('float32')\n",
    "\n",
    "            # Fit GMM\n",
    "            gmm_params = {\n",
    "                'n_components': params['gmm__n_components'],\n",
    "                'covariance_type': params['gmm__covariance_type'],\n",
    "                'random_state': 42\n",
    "            }\n",
    "            gmm = GaussianMixture(**gmm_params)\n",
    "            \n",
    "            if eval_method == 'Silhouette':\n",
    "                cluster_labels = gmm.fit_predict(X_reduced)\n",
    "                score = (silhouette_score(X_reduced, cluster_labels) \n",
    "                         if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels) \n",
    "                         else float('-inf'))\n",
    "            else:  # BIC\n",
    "                gmm.fit(X_reduced)\n",
    "                score = gmm_bic_score(gmm, X_reduced)\n",
    "\n",
    "            print(f\"Parameters: {params}, {eval_method} Score: {score:.4f}\")\n",
    "            results.append({\n",
    "                'lda__n_components': n_lda,\n",
    "                'gmm__n_components': params['gmm__n_components'],\n",
    "                'gmm__covariance_type': params['gmm__covariance_type'],\n",
    "                'score': score,\n",
    "                'dataset': name\n",
    "            })\n",
    "\n",
    "            # Free memory\n",
    "            del X_reduced, gmm\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {name} with params {params}: {e}\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_lda = pd.DataFrame(results)\n",
    "if eval_method == 'BIC':\n",
    "    df_lda['score'] = -df_lda['score']  # Convert negative BIC to positive\n",
    "\n",
    "# Rename columns\n",
    "df_lda = df_lda.rename(columns={\n",
    "    'lda__n_components': 'Number of components (LDA)',\n",
    "    'gmm__n_components': 'Number of clusters (GMM)',\n",
    "    'gmm__covariance_type': 'Covariance Type',\n",
    "    'score': f'{eval_method} score'\n",
    "})\n",
    "\n",
    "# Find best parameters\n",
    "if results:\n",
    "    best_result = df_lda.loc[df_lda[f'{eval_method} score'].idxmax()]\n",
    "    print(\"Best overall:\")\n",
    "    print(f\"Parameters: LDA components={best_result['Number of components (LDA)']}, \"\n",
    "          f\"GMM clusters={best_result['Number of clusters (GMM)']}, \"\n",
    "          f\"Covariance={best_result['Covariance Type']}, Dataset={best_result['dataset']}\")\n",
    "    print(f\"Best {eval_method} score: {best_result[f'{eval_method} score']:.4f}\")\n",
    "\n",
    "# Create and save visualization\n",
    "if not df_lda.empty:\n",
    "    g = sns.catplot(\n",
    "        data=df_lda,\n",
    "        kind=\"bar\",\n",
    "        x=\"Number of components (LDA)\",\n",
    "        y=f\"{eval_method} score\",\n",
    "        hue=\"Number of clusters (GMM)\",\n",
    "        col=\"Covariance Type\",\n",
    "        col_wrap=2,\n",
    "        height=4,\n",
    "        aspect=1.5\n",
    "    )\n",
    "    g.set_axis_labels(\"LDA Components\", f\"{eval_method} Score\")\n",
    "    g.fig.suptitle(f\"LDA vs. GMM on {csv_data}\", y=1.05)\n",
    "    g.set_titles(\"Covariance: {col_name}\")\n",
    "    for ax in g.axes.flat:\n",
    "        ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Save and close figure\n",
    "    plt.savefig(f\"{save_dir}/{csv_data}_lda_dimension_{eval_method}_scores.png\", \n",
    "                dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(g.fig)\n",
    "    del g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "# Configuration\n",
    "param_grid = {\n",
    "    \"lda__n_components\": range(2, 4),\n",
    "    \"gmm__n_components\": range(2, 4),\n",
    "    \"gmm__covariance_type\": ['full', 'tied','diag', 'spherical']\n",
    "}\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "eval_method = 'Silhouette'  # 'BIC', 'Silhouette'\n",
    "results = []\n",
    "\n",
    "# Grid search\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column not in data['df'].columns:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "        continue\n",
    "\n",
    "    X_scaled = data['X_scaled'].astype('float32', copy=False)\n",
    "    labels = data['df'][metadata_column].astype('string', copy=False)\n",
    "\n",
    "    for params in tqdm(ParameterGrid(param_grid), desc=f\"Grid Search in {name}\"):\n",
    "        try:\n",
    "            # Apply LDA\n",
    "            n_lda = params['lda__n_components']\n",
    "            reducer = LDA(n_components=n_lda)\n",
    "            X_reduced = reducer.fit_transform(X_scaled, labels).astype('float32')\n",
    "\n",
    "            # Fit GMM\n",
    "            gmm_params = {\n",
    "                'n_components': params['gmm__n_components'],\n",
    "                'covariance_type': params['gmm__covariance_type'],\n",
    "                'random_state': 42\n",
    "            }\n",
    "            gmm = GaussianMixture(**gmm_params)\n",
    "            \n",
    "            if eval_method == 'Silhouette':\n",
    "                cluster_labels = gmm.fit_predict(X_reduced)\n",
    "                score = (silhouette_score(X_reduced, cluster_labels) \n",
    "                         if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels) \n",
    "                         else float('-inf'))\n",
    "            else:  # BIC\n",
    "                gmm.fit(X_reduced)\n",
    "                score = gmm_bic_score(gmm, X_reduced)\n",
    "\n",
    "            print(f\"Parameters: {params}, {eval_method} Score: {score:.4f}\")\n",
    "            results.append({\n",
    "                'lda__n_components': n_lda,\n",
    "                'gmm__n_components': params['gmm__n_components'],\n",
    "                'gmm__covariance_type': params['gmm__covariance_type'],\n",
    "                'score': score,\n",
    "                'dataset': name\n",
    "            })\n",
    "\n",
    "            # Free memory\n",
    "            del X_reduced, gmm\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {name} with params {params}: {e}\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_lda = pd.DataFrame(results)\n",
    "if eval_method == 'BIC':\n",
    "    df_lda['score'] = -df_lda['score']  # Convert negative BIC to positive\n",
    "\n",
    "# Rename columns\n",
    "df_lda = df_lda.rename(columns={\n",
    "    'lda__n_components': 'Number of components (LDA)',\n",
    "    'gmm__n_components': 'Number of clusters (GMM)',\n",
    "    'gmm__covariance_type': 'Covariance Type',\n",
    "    'score': f'{eval_method} score'\n",
    "})\n",
    "\n",
    "# Find best parameters\n",
    "if results:\n",
    "    best_result = df_lda.loc[df_lda[f'{eval_method} score'].idxmax()]\n",
    "    print(\"Best overall:\")\n",
    "    print(f\"Parameters: LDA components={best_result['Number of components (LDA)']}, \"\n",
    "          f\"GMM clusters={best_result['Number of clusters (GMM)']}, \"\n",
    "          f\"Covariance={best_result['Covariance Type']}, Dataset={best_result['dataset']}\")\n",
    "    print(f\"Best {eval_method} score: {best_result[f'{eval_method} score']:.4f}\")\n",
    "\n",
    "# Create and save visualization\n",
    "if not df_lda.empty:\n",
    "    g = sns.catplot(\n",
    "        data=df_lda,\n",
    "        kind=\"bar\",\n",
    "        x=\"Number of components (LDA)\",\n",
    "        y=f\"{eval_method} score\",\n",
    "        hue=\"Number of clusters (GMM)\",\n",
    "        col=\"Covariance Type\",\n",
    "        col_wrap=2,\n",
    "        height=4,\n",
    "        aspect=1.5\n",
    "    )\n",
    "    g.set_axis_labels(\"LDA Components\", f\"{eval_method} Score\")\n",
    "    g.fig.suptitle(f\"LDA vs. GMM on {csv_data}\", y=1.05)\n",
    "    g.set_titles(\"Covariance: {col_name}\")\n",
    "    \n",
    "    # Add grid and customize bars\n",
    "    for ax in g.axes.flat:\n",
    "        ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)\n",
    "        # Add decimal labels to bars with .3f format\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.3f', label_type='edge', padding=3)\n",
    "    \n",
    "    # Remove default legend and add custom legend at bottom center with title\n",
    "    g._legend.remove()\n",
    "    g.add_legend(loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=len(df_lda['Number of clusters (GMM)'].unique()), title=\"Number of clusters (GMM)\")\n",
    "    \n",
    "    # Save and close figure\n",
    "    plt.savefig(f\"{save_dir}/{csv_data}_lda_dimension_{eval_method}_scores.png\", \n",
    "                dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(g.fig)\n",
    "    del g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
