{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import re\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/jen-hungwang/Documents/mnp-liver/results\"\n",
    "# save_dir = \"/storage/jenhung/results/mnp/\" # Adjusted for Docker environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "csv_dir = Path(\"/home/jen-hungwang/Documents/MNP\")\n",
    "# csv_dir = Path(\"/storage/jenhung/data/mnp_liver\")  # Adjusted for Docker environment\n",
    "hep_path = csv_dir / \"hep\" # hep or huh\n",
    "csv_data = \"df_SingleCell_AO_HEPG2_102912.csv\" # \"df_HUH7_SingleCell_102912.csv\", \"df_HUH7_SingleCell_110341.csv\", \"df_HUH7_SingleCell_191735.csv\"\n",
    "f = hep_path / csv_data\n",
    "\n",
    "df = pd.read_csv(f, sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly drop NC for data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'Metadata_concentration_perliter' \n",
    "value_counts = df[column_name].value_counts()\n",
    "print(f\"Unique values and their counts in column '{column_name}':\")\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame and column_name is defined\n",
    "column_name = 'Metadata_concentration_perliter'\n",
    "\n",
    "# Define the fraction to keep (%)\n",
    "percentage_to_keep = 38\n",
    "\n",
    "# Convert column to string type upfront to avoid repeated conversions and standardize '0' values\n",
    "df[column_name] = df[column_name].astype('string').where(\n",
    "    df[column_name].astype('string') != '0', \n",
    "    df[column_name].astype('float64', errors='ignore').eq(0).astype('string')\n",
    ")\n",
    "\n",
    "# Identify rows where Metadata_concentration_perliter == '0' using boolean indexing\n",
    "zero_mask = df[column_name] == '0'\n",
    "\n",
    "# Calculate number of rows to keep\n",
    "rows_to_keep = int(zero_mask.sum() * percentage_to_keep / 100)\n",
    "print(f\"Number of rows with {column_name} == '0': {zero_mask.sum()}\")\n",
    "print(f\"Number of rows to keep ({percentage_to_keep:.2f}% of zeros): {rows_to_keep}\")\n",
    "\n",
    "# Randomly sample indices of zero rows to keep\n",
    "zero_indices_to_keep = df[zero_mask].sample(n=rows_to_keep, random_state=42).index\n",
    "\n",
    "# Create filtered DataFrame\n",
    "df_filtered = df.loc[zero_indices_to_keep | ~zero_mask]\n",
    "\n",
    "# Reset index in-place\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Verify the new counts\n",
    "value_counts = df_filtered[column_name].value_counts()\n",
    "print(f\"Unique values and their counts in column '{column_name}' after dropping {100 - percentage_to_keep:.2f}% of zeros:\")\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly delete df to free memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_dataframe(df, nan_threshold=0.05):\n",
    "    # Select feature columns using a generator to avoid list storage\n",
    "    feature_columns = [col for col in df.columns if not (col.startswith(('Metadata_', 'Image_')) or col.endswith('_ObjectNumber'))]\n",
    "    print(f\"Number of feature columns: {len(feature_columns)}\")\n",
    "    \n",
    "    # Convert to float32 upfront to reduce memory (assuming numerical data)\n",
    "    X = df[feature_columns].astype('float32', copy=False)\n",
    "    \n",
    "    # Calculate NaN and Inf counts in one pass\n",
    "    nan_counts = X.isna().sum()\n",
    "    inf_counts = np.isinf(X).sum()\n",
    "    \n",
    "    # Identify columns with NaN or Inf exceeding threshold\n",
    "    threshold = X.shape[0] * nan_threshold\n",
    "    invalid_columns = set(nan_counts[nan_counts >= threshold].index).union(\n",
    "        inf_counts[inf_counts >= threshold].index\n",
    "    )\n",
    "    valid_columns = [col for col in feature_columns if col not in invalid_columns]\n",
    "    \n",
    "    # Print columns with NaN or Inf if any\n",
    "    columns_with_nan_or_inf = set(nan_counts[nan_counts > 0].index).union(\n",
    "        inf_counts[inf_counts > 0].index\n",
    "    )\n",
    "    if columns_with_nan_or_inf:\n",
    "        print(\"\\nColumns with at least one NaN or Inf value:\")\n",
    "        print(f\"{'Column':<60} {'NaN Count':>10} {'Inf Count':>10}\")\n",
    "        print(\"-\" * 80)\n",
    "        for col in sorted(columns_with_nan_or_inf):\n",
    "            print(f\"{col:<60} {nan_counts[col]:>10} {inf_counts[col]:>10}\")\n",
    "        print(f\"Total columns with NaN or Inf: {len(columns_with_nan_or_inf)}\")\n",
    "    \n",
    "    print(f\"Number of valid columns: {len(valid_columns)}\")\n",
    "    if not valid_columns:\n",
    "        raise ValueError(\"No valid columns remain after filtering.\")\n",
    "    \n",
    "    # Select valid columns in-place\n",
    "    X.drop(columns=[col for col in X.columns if col not in valid_columns], inplace=True)\n",
    "    \n",
    "    # Replace inf with NaN in-place\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Fill NaN with median in one pass\n",
    "    medians = X.median()\n",
    "    X.fillna(medians, inplace=True)\n",
    "    \n",
    "    # Check for remaining NaN values\n",
    "    nan_count_after_fill = X.isna().sum().sum()\n",
    "    print(f\"\\nNaN count after filling with median: {nan_count_after_fill}\")\n",
    "    if nan_count_after_fill > 0:\n",
    "        print(\"Warning: Some NaN values remain. Filling with zero.\")\n",
    "        X.fillna(0, inplace=True)\n",
    "    \n",
    "    # Check if data is valid\n",
    "    if X.shape[0] == 0 or X.shape[1] == 0:\n",
    "        raise ValueError(\"No rows/columns remain after preprocessing.\")\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X).astype('float32')  # Keep float32 for scaled data\n",
    "    \n",
    "    # Explicitly delete X to free memory\n",
    "    del X\n",
    "    \n",
    "    return X_scaled, valid_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess DataFrame\n",
    "dataframes = {'df': df_filtered}\n",
    "preprocessed_data = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"Original {name} shape: {df.shape}\")\n",
    "    print(f\"Preprocessing {name}...\")\n",
    "    X_scaled, valid_columns = preprocess_dataframe(df)\n",
    "    preprocessed_data[name] = {'X_scaled': X_scaled, 'valid_columns': valid_columns}\n",
    "    print(f\"Preprocessed {name} with {len(valid_columns)} valid columns.\\n\")\n",
    "    \n",
    "    # Delete df_filtered if not needed later\n",
    "    del df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D & 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import umap\n",
    "import seaborn as sns\n",
    "\n",
    "def convert_concentration(value):\n",
    "    if pd.isna(value) or value == \"\":\n",
    "        return 0.0\n",
    "    try:\n",
    "        value = str(value).lower().replace(\" \", \"\")\n",
    "        match = re.match(r'(\\d*\\.?\\d+)([mnu]?g)?', value)\n",
    "        if match:\n",
    "            num = float(match.group(1))\n",
    "            unit = match.group(2) or ''\n",
    "            if unit == 'mg':\n",
    "                return num * 1e-3\n",
    "            elif unit == 'ug':\n",
    "                return num * 1e-6\n",
    "            elif unit == 'ng':\n",
    "                return num * 1e-9\n",
    "            return num  # 'g' or no unit\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def plot_dimensionality_reduction(X_scaled, df, valid_columns, metadata_column, method_name, title, \n",
    "                                  tsne_perplexity=30, n_neighbors=15, min_dist=0.1, continuous=True, \n",
    "                                  n_components=2, save_path=None):\n",
    "    # Convert X_scaled to float32 to reduce memory\n",
    "    X_scaled = X_scaled.astype('float32', copy=False)\n",
    "    \n",
    "    # Initialize reducer and labels\n",
    "    if method_name == 'PCA':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        x_label, y_label = 'PC1', 'PC2'\n",
    "        z_label = 'PC3' if n_components == 3 else None\n",
    "    elif method_name == 't-SNE':\n",
    "        reducer = TSNE(n_components=n_components, perplexity=tsne_perplexity, learning_rate='auto', random_state=42)\n",
    "        x_label, y_label = 't-SNE 1', 't-SNE 2'\n",
    "        z_label = 't-SNE 3' if n_components == 3 else None\n",
    "    elif method_name == 'UMAP':\n",
    "        reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=42)\n",
    "        x_label, y_label = 'UMAP 1', 'UMAP 2'\n",
    "        z_label = 'UMAP 3' if n_components == 3 else None\n",
    "    elif method_name == 'LDA':\n",
    "        labels = df[metadata_column].astype('string', copy=False)\n",
    "        reducer = LDA(n_components=n_components)\n",
    "        x_label, y_label = 'LD1', 'LD2'\n",
    "        z_label = 'LD3' if n_components == 3 else None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method\")\n",
    "\n",
    "    # Perform dimensionality reduction\n",
    "    try:\n",
    "        X_reduced = (reducer.fit_transform(X_scaled, labels) if method_name == 'LDA' \n",
    "                     else reducer.fit_transform(X_scaled)).astype('float32')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {method_name} reduction: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize figure based on n_components\n",
    "    is_3d = n_components == 3\n",
    "    fig = go.Figure()\n",
    "    title_suffix = ' (Continuous)' if continuous else ' (Categorical)'\n",
    "    \n",
    "    # Plotting\n",
    "    if continuous:\n",
    "        concentrations = df[metadata_column].apply(convert_concentration).astype('float32')\n",
    "        if is_3d:\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=X_reduced[:, 0], y=X_reduced[:, 1], z=X_reduced[:, 2], mode='markers',\n",
    "                marker=dict(color=concentrations, colorscale='Viridis', showscale=True, \n",
    "                           colorbar=dict(title=f'{metadata_column} (g)')))\n",
    "            )\n",
    "        else:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X_reduced[:, 0], y=X_reduced[:, 1], mode='markers',\n",
    "                marker=dict(color=concentrations, colorscale='Viridis', showscale=True, \n",
    "                           colorbar=dict(title=f'{metadata_column} (g)')))\n",
    "            )\n",
    "    else:\n",
    "        labels = df[metadata_column].astype('string', copy=False)\n",
    "        unique_labels = sorted(labels.unique(), key=convert_concentration, reverse=True)\n",
    "        color_map = {label: f'rgb({int(r*255)}, {int(g*255)}, {int(b*255)})' \n",
    "                     for label, (r, g, b) in zip(unique_labels, sns.color_palette('tab10', len(unique_labels)))}\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            mask = labels == label\n",
    "            if is_3d:\n",
    "                fig.add_trace(go.Scatter3d(\n",
    "                    x=X_reduced[mask, 0], y=X_reduced[mask, 1], z=X_reduced[mask, 2], \n",
    "                    mode='markers', marker=dict(color=color_map[label], size=3),\n",
    "                    name=str(label), showlegend=True\n",
    "                ))\n",
    "            else:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=X_reduced[mask, 0], y=X_reduced[mask, 1], \n",
    "                    mode='markers', marker=dict(color=color_map[label], size=3),\n",
    "                    name=str(label), showlegend=True\n",
    "                ))\n",
    "\n",
    "        fig.update_layout(showlegend=True, legend=dict(itemsizing='constant'))\n",
    "\n",
    "    # Update layout\n",
    "    layout = dict(\n",
    "        title=f\"{title}{title_suffix} ({'3D' if is_3d else '2D'}, {len(valid_columns)} features)\",\n",
    "        **({'scene': dict(xaxis_title=x_label, yaxis_title=y_label, zaxis_title=z_label)} if is_3d \n",
    "           else {'xaxis_title': x_label, 'yaxis_title': y_label})\n",
    "    )\n",
    "    fig.update_layout(**layout)\n",
    "\n",
    "    # Save plot if save_path is provided\n",
    "    if save_path:\n",
    "        fig.write_image(f\"{save_path}_{'3D' if is_3d else '2D'}.png\", width=1200, height=800)\n",
    "\n",
    "    # Free memory\n",
    "    del X_reduced, concentrations if continuous else labels\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction methods to each DataFrame\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "method = 'LDA'\n",
    "# save_dir = \"/home/jen-hungwang/Desktop/\"\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        # 2D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=2)\n",
    "        plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=2, \n",
    "                                      save_path=save_dir + f\"{csv_data}_{method}\")\n",
    "        \n",
    "        # 3D plots\n",
    "        # plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "        #                                 method, f\"{method} of {csv_data}\", continuous=True, n_components=3)\n",
    "        fig_original = plot_dimensionality_reduction(data['X_scaled'], data['df'], data['valid_columns'], metadata_column,\n",
    "                                      method, f\"{method} of {csv_data}\", continuous=False, n_components=3,\n",
    "                                      save_path=save_dir + f\"{csv_data}_{method}\")\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search LDA Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Function to convert concentration values to numerical for sorting\n",
    "def convert_concentration(val):\n",
    "    try:\n",
    "        val_str = str(val).lower()\n",
    "        if val_str == '0':\n",
    "            return 0.0\n",
    "        for unit in ['g', 'mg', 'ug', 'ng']:\n",
    "            if unit in val_str:\n",
    "                num = float(val_str.replace(unit, ''))\n",
    "                if unit == 'mg':\n",
    "                    num /= 1000\n",
    "                elif unit == 'ug':\n",
    "                    num /= 1_000_000\n",
    "                elif unit == 'ng':\n",
    "                    num /= 1_000_000_000\n",
    "                return num\n",
    "        return float(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return float('-inf')\n",
    "\n",
    "# Configuration\n",
    "range_n_clusters = [2, 3]\n",
    "range_n_components = [2, 3, 4, 5, 6, 7]\n",
    "use_gmm = True\n",
    "gmm_covariance_types = ['full', 'tied', 'diag', 'spherical']  # Changed to list\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column not in data['df'].columns:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "        continue\n",
    "\n",
    "    # Use the DataFrame directly, avoiding copy\n",
    "    df = data['df']\n",
    "    # Standardize '0' values in-place to string '0' for consistency\n",
    "    df[metadata_column] = df[metadata_column].astype('string').where(\n",
    "        pd.to_numeric(df[metadata_column], errors='coerce') != 0, '0'\n",
    "    )\n",
    "    X_scaled = data['X_scaled'].astype('float32', copy=False)  # Ensure float32\n",
    "\n",
    "    for n_comp in range_n_components:\n",
    "        try:\n",
    "            reducer = LDA(n_components=n_comp)\n",
    "            X_reduced = reducer.fit_transform(X_scaled, df[metadata_column])\n",
    "        except ValueError as e:\n",
    "            print(f\"Error with n_components={n_comp} for {name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Iterate over covariance types if using GMM\n",
    "        covariance_iter = gmm_covariance_types if use_gmm else [None]\n",
    "        for gmm_covariance in covariance_iter:\n",
    "            for n_clusters in range_n_clusters:\n",
    "                # Create figure with three subplots\n",
    "                fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 7), gridspec_kw={'width_ratios': [1, 1, 1.5]})\n",
    "\n",
    "                # Clustering\n",
    "                try:\n",
    "                    if use_gmm:\n",
    "                        clusterer = GaussianMixture(\n",
    "                            n_components=n_clusters, random_state=10, covariance_type=gmm_covariance\n",
    "                        )\n",
    "                        algo_name = f\"GMM_{gmm_covariance}\"\n",
    "                    else:\n",
    "                        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                        algo_name = \"KMeans\"\n",
    "\n",
    "                    cluster_labels = clusterer.fit_predict(X_reduced)\n",
    "                    centers = clusterer.means_ if use_gmm else clusterer.cluster_centers_\n",
    "                    centers = centers.astype('float32')  # Reduce memory\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in clustering for {name}, n_components={n_comp}, n_clusters={n_clusters}, {algo_name}: {e}\")\n",
    "                    plt.close(fig)\n",
    "                    continue\n",
    "\n",
    "                silhouette_avg = silhouette_score(X_reduced, cluster_labels)\n",
    "                print(\n",
    "                    f\"For {name}, n_components={n_comp}, n_clusters={n_clusters}, {algo_name}, \"\n",
    "                    f\"silhouette_score={silhouette_avg:.4f}\"\n",
    "                )\n",
    "\n",
    "                # Silhouette Plot (ax1)\n",
    "                ax1.set_xlim([-0.1, 1])\n",
    "                ax1.set_ylim([0, len(X_reduced) + (n_clusters + 1) * 10])\n",
    "                sample_silhouette_values = silhouette_samples(X_reduced, cluster_labels)\n",
    "\n",
    "                y_lower = 10\n",
    "                for i in range(n_clusters):\n",
    "                    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "                    ith_cluster_silhouette_values.sort()\n",
    "                    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "                    y_upper = y_lower + size_cluster_i\n",
    "                    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "                    ax1.fill_betweenx(\n",
    "                        np.arange(y_lower, y_upper),\n",
    "                        0,\n",
    "                        ith_cluster_silhouette_values,\n",
    "                        facecolor=color,\n",
    "                        edgecolor=color,\n",
    "                        alpha=0.7,\n",
    "                    )\n",
    "                    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "                    y_lower = y_upper + 10\n",
    "\n",
    "                ax1.set_title(\"Silhouette Plot\")\n",
    "                ax1.set_xlabel(\"Silhouette Coefficient Values\")\n",
    "                ax1.set_ylabel(\"Cluster Label\")\n",
    "                ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "                ax1.set_yticks([])\n",
    "                ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "                # Cluster Scatter Plot (ax2)\n",
    "                colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "                ax2.scatter(\n",
    "                    X_reduced[:, 0], X_reduced[:, 1], marker=\".\", s=30, lw=0,\n",
    "                    alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "                )\n",
    "                ax2.scatter(\n",
    "                    centers[:, 0], centers[:, 1], marker=\"o\", c=\"white\", alpha=1,\n",
    "                    s=200, edgecolor=\"k\"\n",
    "                )\n",
    "                for i, c in enumerate(centers):\n",
    "                    ax2.scatter(c[0], c[1], marker=f\"${i}$\", alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "                ax2.set_title(f\"Cluster Visualization ({algo_name})\")\n",
    "                ax2.set_xlabel(\"Feature Space (1st Dimension)\")\n",
    "                ax2.set_ylabel(\"Feature Space (2nd Dimension)\")\n",
    "\n",
    "                # Concentration Distribution Plot (ax3)\n",
    "                unique_clusters = np.unique(cluster_labels[cluster_labels != -1])\n",
    "                concentration_values = df[metadata_column].unique()\n",
    "                converted_values = np.array([convert_concentration(val) for val in concentration_values])\n",
    "                sort_indices = np.argsort(converted_values)[::-1]\n",
    "                sorted_concentration_values = concentration_values[sort_indices]\n",
    "\n",
    "                for idx, cluster in enumerate(unique_clusters):\n",
    "                    cluster_mask = cluster_labels == cluster\n",
    "                    cluster_concentrations = df[metadata_column][cluster_mask]\n",
    "                    counts = np.zeros(len(sorted_concentration_values), dtype='int32')  # Use int32\n",
    "                    for i, conc in enumerate(sorted_concentration_values):\n",
    "                        counts[i] = np.sum(cluster_concentrations == conc)\n",
    "\n",
    "                    ax3.bar(\n",
    "                        np.arange(len(sorted_concentration_values)) + idx * 0.2,\n",
    "                        counts,\n",
    "                        width=0.2,\n",
    "                        label=f'Cluster {cluster}',\n",
    "                        color=cm.nipy_spectral(float(cluster) / n_clusters),\n",
    "                        alpha=0.7\n",
    "                    )\n",
    "\n",
    "                ax3.set_title(\"Concentration Distribution Across Clusters\")\n",
    "                ax3.set_xlabel(\"Concentration Levels (High to Low)\")\n",
    "                ax3.set_ylabel(\"Count\")\n",
    "                ax3.set_xticks(np.arange(len(sorted_concentration_values)))\n",
    "                ax3.set_xticklabels(sorted_concentration_values, rotation=45, ha='right')\n",
    "                ax3.legend()\n",
    "\n",
    "                # Overall figure title\n",
    "                plt.suptitle(\n",
    "                    f\"{csv_data} - Analysis with {algo_name}, Silhouette={silhouette_avg:.4f}, \"\n",
    "                    f\"n_clusters={n_clusters}, LDA={n_comp}D\",\n",
    "                    fontsize=14, fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "                # Save and close plot\n",
    "                filename = f\"{algo_name}_{n_clusters}clusters_{n_comp}D_{csv_data}.png\"\n",
    "                plt.savefig(os.path.join(save_dir, filename), bbox_inches='tight', dpi=300)\n",
    "                plt.close(fig)  # Free memory\n",
    "                del fig  # Explicitly delete figure\n",
    "\n",
    "        # Free memory after each n_components iteration\n",
    "        del X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Grid Search: GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "# Configuration\n",
    "param_grid = {\n",
    "    \"lda__n_components\": range(2, 8),\n",
    "    \"gmm__n_components\": range(2, 8),\n",
    "    \"gmm__covariance_type\": ['full', 'tied', 'diag', 'spherical']\n",
    "}\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "eval_method = 'Silhouette'  # 'BIC', 'Silhouette'\n",
    "results = []\n",
    "\n",
    "# Grid search\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column not in data['df'].columns:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "        continue\n",
    "\n",
    "    X_scaled = data['X_scaled'].astype('float32', copy=False)\n",
    "    labels = data['df'][metadata_column].astype('string', copy=False)\n",
    "\n",
    "    for params in tqdm(ParameterGrid(param_grid), desc=f\"Grid Search in {name}\"):\n",
    "        try:\n",
    "            # Apply LDA\n",
    "            n_lda = params['lda__n_components']\n",
    "            reducer = LDA(n_components=n_lda)\n",
    "            X_reduced = reducer.fit_transform(X_scaled, labels).astype('float32')\n",
    "\n",
    "            # Fit GMM\n",
    "            gmm_params = {\n",
    "                'n_components': params['gmm__n_components'],\n",
    "                'covariance_type': params['gmm__covariance_type'],\n",
    "                'random_state': 42\n",
    "            }\n",
    "            gmm = GaussianMixture(**gmm_params)\n",
    "            \n",
    "            if eval_method == 'Silhouette':\n",
    "                cluster_labels = gmm.fit_predict(X_reduced)\n",
    "                score = (silhouette_score(X_reduced, cluster_labels) \n",
    "                         if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels) \n",
    "                         else float('-inf'))\n",
    "            else:  # BIC\n",
    "                gmm.fit(X_reduced)\n",
    "                score = gmm_bic_score(gmm, X_reduced)\n",
    "\n",
    "            print(f\"Parameters: {params}, {eval_method} Score: {score:.4f}\")\n",
    "            results.append({\n",
    "                'lda__n_components': n_lda,\n",
    "                'gmm__n_components': params['gmm__n_components'],\n",
    "                'gmm__covariance_type': params['gmm__covariance_type'],\n",
    "                'score': score,\n",
    "                'dataset': name\n",
    "            })\n",
    "\n",
    "            # Free memory\n",
    "            del X_reduced, gmm\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {name} with params {params}: {e}\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_lda = pd.DataFrame(results)\n",
    "if eval_method == 'BIC':\n",
    "    df_lda['score'] = -df_lda['score']  # Convert negative BIC to positive\n",
    "\n",
    "# Rename columns\n",
    "df_lda = df_lda.rename(columns={\n",
    "    'lda__n_components': 'Number of components (LDA)',\n",
    "    'gmm__n_components': 'Number of clusters (GMM)',\n",
    "    'gmm__covariance_type': 'Covariance Type',\n",
    "    'score': f'{eval_method} score'\n",
    "})\n",
    "\n",
    "# Find best parameters\n",
    "if results:\n",
    "    best_result = df_lda.loc[df_lda[f'{eval_method} score'].idxmax()]\n",
    "    print(\"Best overall:\")\n",
    "    print(f\"Parameters: LDA components={best_result['Number of components (LDA)']}, \"\n",
    "          f\"GMM clusters={best_result['Number of clusters (GMM)']}, \"\n",
    "          f\"Covariance={best_result['Covariance Type']}, Dataset={best_result['dataset']}\")\n",
    "    print(f\"Best {eval_method} score: {best_result[f'{eval_method} score']:.4f}\")\n",
    "\n",
    "# Create and save visualization\n",
    "if not df_lda.empty:\n",
    "    g = sns.catplot(\n",
    "        data=df_lda,\n",
    "        kind=\"bar\",\n",
    "        x=\"Number of components (LDA)\",\n",
    "        y=f\"{eval_method} score\",\n",
    "        hue=\"Number of clusters (GMM)\",\n",
    "        col=\"Covariance Type\",\n",
    "        col_wrap=2,\n",
    "        height=4,\n",
    "        aspect=1.5\n",
    "    )\n",
    "    g.set_axis_labels(\"LDA Components\", f\"{eval_method} Score\")\n",
    "    g.fig.suptitle(f\"LDA vs. GMM on {csv_data}\", y=1.05)\n",
    "    g.set_titles(\"Covariance: {col_name}\")\n",
    "    for ax in g.axes.flat:\n",
    "        ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Save and close figure\n",
    "    plt.savefig(f\"{save_dir}/{csv_data}_lda_dimension_{eval_method}_scores.png\", \n",
    "                dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(g.fig)\n",
    "    del g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. LDA Dimension vs. GMM Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "param_grid = {\n",
    "    \"lda__n_components\": range(2, 8),  # Try different LDA dimensions\n",
    "    \"gmm__n_components\": range(2, 8),\n",
    "}\n",
    "\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "eval_method = 'Silhouette' # 'BIC', 'Silhouette'\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column not in data['df'].columns:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "        continue\n",
    "\n",
    "    X_scaled = data['X_scaled']\n",
    "    labels = data['df'][metadata_column].astype(str)\n",
    "\n",
    "    for params in tqdm(ParameterGrid(param_grid), desc=f\"Grid Search in {name}\"):\n",
    "        try:\n",
    "            # Apply LDA with current n_components\n",
    "            n_lda = params['lda__n_components']\n",
    "            reducer = LDA(n_components=n_lda)\n",
    "            X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "\n",
    "            # Fit GMM with current parameters\n",
    "            gmm_params = {\n",
    "                'n_components': params['gmm__n_components'],\n",
    "                'covariance_type': 'tied'\n",
    "            }\n",
    "            gmm = GaussianMixture(**gmm_params)\n",
    "            \n",
    "            if eval_method == 'Silhouette':\n",
    "                cluster_labels = gmm.fit_predict(X_reduced)\n",
    "                if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels):\n",
    "                    score = silhouette_score(X_reduced, cluster_labels)\n",
    "                else:\n",
    "                    score = float('-inf')\n",
    "            elif eval_method == 'BIC':\n",
    "                gmm.fit(X_reduced)\n",
    "                score = gmm_bic_score(gmm, X_reduced)\n",
    "\n",
    "            print(f\"Parameters: {params}, {eval_method} Score: {score}\")\n",
    "\n",
    "            results.append((params, score))\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {name} with params {params}: {e}\")\n",
    "\n",
    "# Find best parameters globally or per dataset\n",
    "best_result = max(results, key=lambda x: x[1])\n",
    "print(\"Best overall:\")\n",
    "print(\"Parameters:\", best_result[0])\n",
    "print(f\"Best {eval_method} score:\", best_result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_lda = pd.DataFrame([\n",
    "    {\n",
    "        \"param_n_components\": params[\"lda__n_components\"],\n",
    "        \"param_n_clusters\": params[\"gmm__n_components\"],\n",
    "        \"mean_test_score\": score\n",
    "    }\n",
    "    for params, score in results\n",
    "])\n",
    "\n",
    "if eval_method == 'BIC':\n",
    "    # Convert negative BIC to positive BIC (since gmm_bic_score returns negative BIC)\n",
    "    df_lda[\"mean_test_score\"] = -df_lda[\"mean_test_score\"]\n",
    "\n",
    "# Rename columns to match desired output\n",
    "df_lda = df_lda.rename(\n",
    "    columns={\n",
    "        \"param_n_components\": \"Number of components (LDA)\",\n",
    "        \"param_n_clusters\": \"Number of clusters (GMM)\",\n",
    "        \"mean_test_score\": f\"{eval_method} score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort by BIC score and display top 5\n",
    "df_lda.sort_values(by=f\"{eval_method} score\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=df_lda,\n",
    "    kind=\"bar\",\n",
    "    x=\"Number of components (LDA)\",\n",
    "    y=f\"{eval_method} score\",\n",
    "    hue=\"Number of clusters (GMM)\",\n",
    ")\n",
    "\n",
    "# g.ax.set_ylim(40, 45)  # Set y-axis limit to 26,000 as requested\n",
    "g.ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)  # Add a grid for better readability\n",
    "\n",
    "# Add title with margin below\n",
    "plt.title(f\"LDA Number of Components vs. GMM Number of Clusters on {csv_data}\", pad=30)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f\"{save_dir}/{csv_data}_lda_dimension_{eval_method}_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GMM Clusters vs. GMM Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_components\": range(2, 3), # (2,8)\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "n_comp = 3  # Number of components for LDA\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        labels = data['df'][metadata_column].astype(str)\n",
    "        reducer = LDA(n_components=n_comp)\n",
    "        X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over parameter combinations with a progress bar\n",
    "for params in tqdm(ParameterGrid(param_grid), desc=\"Grid Search Progress\"):\n",
    "    gmm = GaussianMixture(**params)\n",
    "    gmm.fit(X_reduced)\n",
    "    score = gmm_bic_score(gmm, X_reduced)  # Compute BIC on the full dataset\n",
    "    print(f\"Parameters: {params}, Score (negative BIC): {score}\")\n",
    "    results.append((params, score))\n",
    "\n",
    "# Find the best parameters\n",
    "best_result = max(results, key=lambda x: x[1])\n",
    "best_params = best_result[0]\n",
    "best_score = best_result[1]\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score (negative BIC):\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_bic = pd.DataFrame([\n",
    "    {\n",
    "        \"param_n_components\": params[\"n_components\"],\n",
    "        \"param_covariance_type\": params[\"covariance_type\"],\n",
    "        \"mean_test_score\": -abs(score)\n",
    "    }\n",
    "    for params, score in results\n",
    "])\n",
    "\n",
    "# Convert negative BIC to positive BIC (since gmm_bic_score returns negative BIC)\n",
    "df_bic[\"mean_test_score\"] = -df_bic[\"mean_test_score\"]\n",
    "\n",
    "# Rename columns to match desired output\n",
    "df_bic = df_bic.rename(\n",
    "    columns={\n",
    "        \"param_n_components\": \"Number of clusters\",\n",
    "        \"param_covariance_type\": \"Type of covariance\",\n",
    "        \"mean_test_score\": \"BIC score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort by BIC score and display top 5\n",
    "df_bic.sort_values(by=\"BIC score\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=df_bic,\n",
    "    kind=\"bar\",\n",
    "    x=\"Number of clusters\",\n",
    "    y=\"BIC score\",\n",
    "    hue=\"Type of covariance\",\n",
    "    \n",
    ")\n",
    "\n",
    "# g.ax.set_ylim(250000, 285000)  # Set y-axis limit to 26,000 as requested\n",
    "# g.ax.set_ylim(400000, 450000)\n",
    "g.ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)  # Add a grid for better readability\n",
    "\n",
    "# Add title with margin below\n",
    "plt.title(f\"GMM Number of Clusters by GMM Covariance Types on {csv_data}\", pad=30)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f\"{save_dir}/{csv_data}_bic_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "param_grid = {\n",
    "    \"n_components\": range(2, 8),  # Silhouette score requires at least 2 clusters\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "n_comp = 3  # Number of components for LDA\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        labels = data['df'][metadata_column].astype(str)\n",
    "        reducer = LDA(n_components=n_comp)\n",
    "        X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in tqdm(ParameterGrid(param_grid), desc=\"Grid Search Progress\"):\n",
    "    gmm = GaussianMixture(**params, random_state=42)\n",
    "    cluster_labels = gmm.fit_predict(X_reduced)\n",
    "    # Silhouette score is only valid if there is more than 1 cluster\n",
    "    if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels):\n",
    "        score = silhouette_score(X_reduced, cluster_labels)\n",
    "    else:\n",
    "        score = float('-inf')\n",
    "    print(f\"Parameters: {params}, Silhouette Score: {score}\")\n",
    "    results.append((params, score))\n",
    "\n",
    "# Find the best parameters\n",
    "best_result = max(results, key=lambda x: x[1])\n",
    "best_params = best_result[0]\n",
    "best_score = best_result[1]\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best Silhouette Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_silhouette = pd.DataFrame([\n",
    "    {\n",
    "        \"param_n_components\": params[\"n_components\"],\n",
    "        \"param_covariance_type\": params[\"covariance_type\"],\n",
    "        \"mean_test_score\": score\n",
    "    }\n",
    "    for params, score in results\n",
    "])\n",
    "\n",
    "# Rename columns to match desired output\n",
    "df_silhouette = df_silhouette.rename(\n",
    "    columns={\n",
    "        \"param_n_components\": \"Number of clusters\",\n",
    "        \"param_covariance_type\": \"Type of covariance\",\n",
    "        \"mean_test_score\": \"Silhouette score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort by BIC score and display top 5\n",
    "df_silhouette.sort_values(by=\"Silhouette score\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=df_silhouette,\n",
    "    kind=\"bar\",\n",
    "    x=\"Number of clusters\",\n",
    "    y=\"Silhouette score\",\n",
    "    hue=\"Type of covariance\",\n",
    ")\n",
    "\n",
    "# g.ax.set_ylim(250000, 285000)  # Set y-axis limit to 26,000 as requested\n",
    "# g.ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)  # Add a grid for better readability\n",
    "\n",
    "# Add title with margin below\n",
    "plt.title(f\"GMM Number of Clusters vs. GMM Covariance Types on {csv_data}\", pad=30)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f\"{save_dir}/{csv_data}_silhouette_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LDA Dimension vs. GMM Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define parameter grid for GMM (only covariance_type)\n",
    "gmm_param_grid = {\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "\n",
    "# Metadata column for LDA supervision\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "gmm_n_components = 3  # Fixed number of components for GMM\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Process each dataset in preprocessed_data\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        labels = data['df'][metadata_column].astype(str)\n",
    "        \n",
    "        # Determine max n_components for LDA (min of n_classes-1, n_features)\n",
    "        n_classes = len(np.unique(labels))\n",
    "        n_features = X_scaled.shape[1]\n",
    "        max_n_components = min(n_classes - 1, n_features)\n",
    "        print(\"Max n_components for LDA:\", max_n_components)\n",
    "        \n",
    "        if max_n_components < 1:\n",
    "            print(f\"Warning: Insufficient classes or features for LDA in {csv_data}.\")\n",
    "            continue\n",
    "            \n",
    "        # Define parameter grid for LDA\n",
    "        lda_param_grid = {\n",
    "            \"n_components\": range(1, max_n_components + 1)\n",
    "        }\n",
    "        \n",
    "        # Grid search over LDA parameters\n",
    "        for lda_params in ParameterGrid(lda_param_grid):\n",
    "            # Apply LDA\n",
    "            reducer = LinearDiscriminantAnalysis(**lda_params)\n",
    "            try:\n",
    "                X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "                \n",
    "                # Grid search over GMM covariance_type\n",
    "                for gmm_params in tqdm(ParameterGrid(gmm_param_grid), desc=f\"LDA n_components={lda_params['n_components']}: GMM Grid Search\"):\n",
    "                    gmm = GaussianMixture(n_components=gmm_n_components, **gmm_params, random_state=42)\n",
    "                    cluster_labels = gmm.fit_predict(X_reduced)\n",
    "                    \n",
    "                    # Compute silhouette score (valid if >1 cluster, no noise labels)\n",
    "                    if len(set(cluster_labels)) > 1 and -1 not in set(cluster_labels):\n",
    "                        score = silhouette_score(X_reduced, cluster_labels)\n",
    "                    else:\n",
    "                        score = float('-inf')\n",
    "                        \n",
    "                    print(f\"Dataset: {csv_data}, LDA Parameters: {lda_params}, GMM Parameters: {gmm_params}, Silhouette Score: {score}\")\n",
    "                    results.append({\n",
    "                        \"dataset\": csv_data,\n",
    "                        \"lda_params\": lda_params,\n",
    "                        \"gmm_params\": gmm_params,\n",
    "                        \"score\": score\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error for dataset {csv_data}, LDA params={lda_params}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {csv_data}.\")\n",
    "\n",
    "# Find best result\n",
    "if results:\n",
    "    best_result = max(results, key=lambda x: x[\"score\"])\n",
    "    print(f\"Best Dataset: {best_result['dataset']}\")\n",
    "    print(f\"Best LDA Parameters: {best_result['lda_params']}\")\n",
    "    print(f\"Best GMM Parameters: {best_result['gmm_params']}\")\n",
    "    print(f\"Best Silhouette Score: {best_result['score']}\")\n",
    "else:\n",
    "    print(\"No valid results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_silhouette = pd.DataFrame([\n",
    "    {\n",
    "        \"dataset\": result[\"dataset\"],\n",
    "        \"param_lda_n_components\": result[\"lda_params\"][\"n_components\"],\n",
    "        \"param_gmm_covariance_type\": result[\"gmm_params\"][\"covariance_type\"],\n",
    "        \"mean_test_score\": result[\"score\"]\n",
    "    }\n",
    "    for result in results\n",
    "])\n",
    "\n",
    "# Rename columns to match desired output\n",
    "df_silhouette = df_silhouette.rename(\n",
    "    columns={\n",
    "        \"param_lda_n_components\": \"Number of Components (LDA)\",\n",
    "        \"param_gmm_covariance_type\": \"Type of Covariance (GMM)\",\n",
    "        \"mean_test_score\": \"Silhouette score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort by silhouette score (descending) and display top 5\n",
    "df_silhouette.sort_values(by=\"Silhouette score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the catplot\n",
    "g = sns.catplot(\n",
    "    data=df_silhouette,\n",
    "    kind=\"bar\",\n",
    "    x=\"Number of Components (LDA)\",\n",
    "    y=\"Silhouette score\",\n",
    "    hue=\"Type of Covariance (GMM)\",\n",
    ")\n",
    "\n",
    "# g.ax.set_ylim(250000, 285000)  # Set y-axis limit to 26,000 as requested\n",
    "# g.ax.grid(True, axis='y', linestyle=\"--\", alpha=0.7)  # Add a grid for better readability\n",
    "\n",
    "# Add title with margin below\n",
    "plt.title(f\"LDA Number of Components vs. GMM Covariance Types ({gmm_n_components} Clusters) on {csv_data}\", pad=30)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f\"{save_dir}/{csv_data}_LDA_DimvsGMM_covariance_silhouette_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. No LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_components\": range(2, 7),\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "# n_comp = 3  # Number of components for LDA\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        # labels = data['df'][metadata_column].astype(str)\n",
    "        # reducer = LDA(n_components=n_comp)\n",
    "        # X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over parameter combinations with a progress bar\n",
    "for params in tqdm(ParameterGrid(param_grid), desc=\"Grid Search Progress\"):\n",
    "    gmm = GaussianMixture(**params)\n",
    "    gmm.fit(X_scaled)\n",
    "    score = gmm_bic_score(gmm, X_scaled)  # Compute BIC on the full dataset\n",
    "    print(f\"Parameters: {params}, Score (negative BIC): {score}\")\n",
    "    results.append((params, score))\n",
    "\n",
    "# Find the best parameters\n",
    "best_result = max(results, key=lambda x: x[1])\n",
    "best_params = best_result[0]\n",
    "best_score = best_result[1]\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score (negative BIC):\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Grid Search: Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clustering to reduced data from PCA, LDA, t-SNE, and UMAP\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative', 'GaussianMixture']\n",
    "reduction_methods = ['LDA']\n",
    "# reduction_methods = ['PCA', 'LDA', 't-SNE', 'UMAP']\n",
    "n_components_list = [2, 3]\n",
    "n_clusters_list = [3, 4, 5, 6, 7, 8]\n",
    "save_dir = \"/home/jen-hungwang/Desktop/eval/\"\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "\n",
    "        for reduction_method in reduction_methods:\n",
    "            for n_components in n_components_list:\n",
    "                # Compute reduced data based on the method\n",
    "                if reduction_method == 'PCA':\n",
    "                    reducer = PCA(n_components=n_components)\n",
    "                    X_reduced = reducer.fit_transform(X_scaled)\n",
    "                elif reduction_method == 'LDA':\n",
    "                    labels = data['df'][metadata_column].astype(str)\n",
    "                    reducer = LDA(n_components=n_components)\n",
    "                    X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "                elif reduction_method == 't-SNE':\n",
    "                    reducer = TSNE(n_components=n_components, perplexity=30, learning_rate=200, random_state=42)\n",
    "                    X_reduced = reducer.fit_transform(X_scaled)\n",
    "                elif reduction_method == 'UMAP':\n",
    "                    reducer = umap.UMAP(n_components=n_components, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "                    X_reduced = reducer.fit_transform(X_scaled)\n",
    "\n",
    "                for cluster_method in clustering_methods:\n",
    "                    if cluster_method in ['KMeans', 'Agglomerative', 'GaussianMixture']:\n",
    "                        for n_clusters in n_clusters_list:\n",
    "                            labels = perform_clustering(\n",
    "                                X_reduced, \n",
    "                                None, \n",
    "                                metadata_column, \n",
    "                                cluster_method, \n",
    "                                reduction_method, \n",
    "                                n_clusters=n_clusters, \n",
    "                                save_path=save_dir + f\"{name}_{reduction_method}_{cluster_method}_{n_components}D_{n_clusters}clusters\"\n",
    "                            )\n",
    "                            print(f\"{cluster_method} clustering labels on {reduction_method} for {name} ({n_components}D, {n_clusters} clusters): {np.unique(labels)}\")\n",
    "                    elif cluster_method == 'DBSCAN':\n",
    "                        labels = perform_clustering(\n",
    "                            X_reduced, \n",
    "                            None, \n",
    "                            metadata_column, \n",
    "                            cluster_method, \n",
    "                            reduction_method, \n",
    "                            eps=0.5, \n",
    "                            min_samples=5, \n",
    "                            save_path=save_dir + f\"{name}_{reduction_method}_{cluster_method}_{n_components}D\"\n",
    "                        )\n",
    "                        print(f\"{cluster_method} clustering labels on {reduction_method} for {name} ({n_components}D): {np.unique(labels)}\")\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "figs       = [fig_original, fig_reduced]\n",
    "fig_labels = ['Original classes',\n",
    "              'Cluster with dimensionality reduction']\n",
    "num_plots = 2\n",
    "\n",
    "# -- 1. build a 3-column subplot skeleton, each cell is 3-D --------------\n",
    "combined_fig = make_subplots(\n",
    "    rows=1, cols=num_plots,\n",
    "    specs=[[{\"type\": \"scene\"}]*num_plots],               # 3× Scatter3d panels\n",
    "    subplot_titles=fig_labels,\n",
    "    horizontal_spacing=0.07                      # little gap between plots\n",
    ")\n",
    "\n",
    "# -- 2. copy every trace into the right cell, tweak the marker size -------\n",
    "for col, (fig, label) in enumerate(zip(figs, fig_labels), start=1):\n",
    "    for trace in fig.data:\n",
    "        # make dots smaller (overwrite whatever size was there)\n",
    "        if hasattr(trace, \"marker\"):          # safety check\n",
    "            trace.marker.size = 3\n",
    "        # prefix trace names with panel label so the legend is explicit\n",
    "        trace.name = f\"{trace.name}\"\n",
    "        combined_fig.add_trace(trace, row=1, col=col)\n",
    "\n",
    "# -- 3. carry over each panel’s axis titles -------------------------------\n",
    "for col, fig in enumerate(figs, start=1):\n",
    "    if hasattr(fig.layout, \"scene\"):  # 3-D source figure\n",
    "        tgt_scene = \"scene\" if col == 1 else f\"scene{col}\"\n",
    "        combined_fig.update_layout({\n",
    "            tgt_scene: dict(\n",
    "                xaxis_title = fig.layout.scene.xaxis.title.text,\n",
    "                yaxis_title = fig.layout.scene.yaxis.title.text,\n",
    "                zaxis_title = fig.layout.scene.zaxis.title.text\n",
    "            )\n",
    "        })\n",
    "    else:                             # 2-D source figure (just in case)\n",
    "        combined_fig.update_xaxes(title_text=fig.layout.xaxis.title.text,\n",
    "                                  row=1, col=col)\n",
    "        combined_fig.update_yaxes(title_text=fig.layout.yaxis.title.text,\n",
    "                                  row=1, col=col)\n",
    "\n",
    "# -- 4. overall figure cosmetics ------------------------------------------\n",
    "combined_fig.update_layout(\n",
    "    title=\"Combined clustering results\",\n",
    "    height=500, width=1350,\n",
    "    legend=dict(itemsizing=\"constant\")   # keep legend entry size compact\n",
    ")\n",
    "\n",
    "combined_fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function for clustering results\n",
    "def evaluate_clustering(X_reduced, labels, df, metadata_column):\n",
    "    # Convert concentration to categorical labels as ground truth\n",
    "    true_labels = df[metadata_column].astype(str)\n",
    "    \n",
    "    # Silhouette Score\n",
    "    silhouette_avg = silhouette_score(X_reduced, labels) if len(np.unique(labels)) > 1 else None\n",
    "    \n",
    "    # Adjusted Rand Score (requires true labels)\n",
    "    adjusted_rand = adjusted_rand_score(true_labels, labels) if len(np.unique(labels)) > 1 else None\n",
    "    \n",
    "    # Calinski-Harabasz Score\n",
    "    ch_score = calinski_harabasz_score(X_reduced, labels) if len(np.unique(labels)) > 1 else None\n",
    "    \n",
    "    print(f\"Evaluation Metrics for Clustering:\")\n",
    "    print(f\"Silhouette Score: {silhouette_avg:.4f}\" if silhouette_avg is not None else \"Silhouette Score: N/A (single cluster)\")\n",
    "    print(f\"Adjusted Rand Score: {adjusted_rand:.4f}\" if adjusted_rand is not None else \"Adjusted Rand Score: N/A (single cluster or no true labels)\")\n",
    "    print(f\"Calinski-Harabasz Score: {ch_score:.4f}\" if ch_score is not None else \"Calinski-Harabasz Score: N/A (single cluster)\")\n",
    "    return silhouette_avg, adjusted_rand, ch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering results for each reduction method, clustering method, and dataset\n",
    "metadata_column = 'Metadata_concentration_perliter'\n",
    "clustering_methods = ['KMeans', 'DBSCAN', 'Agglomerative']\n",
    "reduction_methods = ['LDA']\n",
    "# reduction_methods = ['PCA', 'LDA', 't-SNE', 'UMAP']\n",
    "n_components = 3\n",
    "save_dir = \"/home/jen-hungwang/Desktop/eval/\"\n",
    "\n",
    "for name, data in preprocessed_data.items():\n",
    "    if metadata_column in data['df'].columns:\n",
    "        X_scaled = data['X_scaled']\n",
    "        \n",
    "        for reduction_method in reduction_methods:\n",
    "            # Compute reduced data based on the method\n",
    "            if reduction_method == 'PCA':\n",
    "                reducer = PCA(n_components=n_components)\n",
    "                X_reduced = reducer.fit_transform(X_scaled)\n",
    "            elif reduction_method == 'LDA':\n",
    "                labels = data['df'][metadata_column].astype(str)\n",
    "                reducer = LDA(n_components=n_components)\n",
    "                X_reduced = reducer.fit_transform(X_scaled, labels)\n",
    "            elif reduction_method == 't-SNE':\n",
    "                reducer = TSNE(n_components=n_components, perplexity=30, learning_rate=200, random_state=42)\n",
    "                X_reduced = reducer.fit_transform(X_scaled)\n",
    "            elif reduction_method == 'UMAP':\n",
    "                reducer = umap.UMAP(n_components=n_components, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "                X_reduced = reducer.fit_transform(X_scaled)\n",
    "            \n",
    "            for cluster_method in clustering_methods:\n",
    "                if cluster_method == 'KMeans' or cluster_method == 'Agglomerative':\n",
    "                    labels = perform_clustering(X_reduced, None, metadata_column, cluster_method, reduction_method, n_clusters=3, save_path=save_dir + f\"{name}_{reduction_method}_{cluster_method}\")\n",
    "                elif cluster_method == 'DBSCAN':\n",
    "                    labels = perform_clustering(X_reduced, None, metadata_column, cluster_method, reduction_method, eps=0.5, min_samples=5, save_path=save_dir + f\"{name}_{reduction_method}_{cluster_method}\")\n",
    "                print(f\"\\nEvaluating {cluster_method} clustering on {reduction_method} for {name}:\")\n",
    "                evaluate_clustering(X_reduced, labels, data['df'], metadata_column)\n",
    "    else:\n",
    "        print(f\"Warning: {metadata_column} not found in {name}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
